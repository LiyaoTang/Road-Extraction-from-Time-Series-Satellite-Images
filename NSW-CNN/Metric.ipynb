{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Metric:\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    size = 0\n",
    "    \n",
    "    def accumulate(self,pred, Y):\n",
    "        self.true_neg  += np.logical_and(pred == Y, np.logical_not(Y)).sum()\n",
    "        self.false_neg += np.logical_and(pred != Y, np.logical_not(Y)).sum()\n",
    "        self.true_pos  += np.logical_and(pred == Y, Y).sum()\n",
    "        self.false_pos += np.logical_and(pred != Y, Y).sum()\n",
    "        self.size += Y.size\n",
    "        \n",
    "    def cal_metric(self):\n",
    "        true_pos  = self.true_pos\n",
    "        false_pos = self.false_pos\n",
    "        true_neg  = self.true_neg\n",
    "        false_neg = self.false_neg\n",
    "        \n",
    "        result = {}\n",
    "        \n",
    "        try: # metric for positive\n",
    "            pos_recall = true_pos / (true_pos + false_neg)\n",
    "            result['pos_recall'] = pos_recall\n",
    "\n",
    "            pos_precision = true_pos / (true_pos + false_pos)\n",
    "            result['pos_precision'] = pos_precision\n",
    "    \n",
    "            pos_F1 = 2*(pos_recall*pos_precision) / (pos_recall+pos_precision)\n",
    "            result['pos_F1'] = pos_F1\n",
    "        except:\n",
    "            print(\"Error in F1 score for Positive\")            \n",
    "        \n",
    "        try: # metric for negative\n",
    "            neg_precision = true_neg / (true_neg + false_neg)\n",
    "            result['neg_precision'] = neg_precision\n",
    "            \n",
    "            neg_recall = true_neg / (true_neg + false_pos)\n",
    "            result['neg_recall'] = neg_recall\n",
    "\n",
    "            neg_F1 = 2*(neg_recall*neg_precision) / (neg_recall+neg_precision)\n",
    "            result['neg_F1'] = neg_F1\n",
    "        except:\n",
    "            print(\"Error in F1 score for Negative\") \n",
    "            \n",
    "        try: # accuracy\n",
    "            accuracy = (true_pos + true_neg) / self.size\n",
    "            result['accuracy'] = accuracy\n",
    "        except:\n",
    "            print(\"Error in accuracy\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"%-9s = %d\\n%-9s = %d\\n%-9s = %d\\n%-9s = %d\\nsize = %d\"\n",
    "                         % ('true_pos', self.true_pos, 'false_pos', self.false_pos,\n",
    "                            'true_neg', self.true_neg, 'false_neg', self.false_neg, self.size))\n",
    "        \n",
    "        result = self.cal_metric()\n",
    "        for key in result.keys():\n",
    "            print(\"%-13s = %s\" % (key, result[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
