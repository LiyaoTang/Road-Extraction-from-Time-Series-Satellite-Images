{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import sklearn.linear_model as sklm\n",
    "import sklearn.metrics as skmt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.append('Metric/')\n",
    "sys.path.append('../Visualization/')\n",
    "sys.path.append('../Data_Preprocessing//')\n",
    "from Metric import *\n",
    "from Visualization import *\n",
    "from Data_Extractor import *\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "path_image_dir = \"../Data/090085/\"\n",
    "path_road_mask = \"../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/road_mask.tif\"\n",
    "\n",
    "classifier_type = 'FCN'\n",
    "path_model_dir = \"./Segmentation-FCN/Result/Inception/Incep_3-32;1-32|3-64;1-64_m_weight_bn_p0_e20_r0/\"\n",
    "model_name = \"Incep_3-32;1-32|3-64;1-64_m_weight_bn_p0_e20_r0\"\n",
    "size = 128\n",
    "step = 16\n",
    "\n",
    "path_pred_dir = \"./Time-Series/Pred/\"\n",
    "\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "use_norm = True\n",
    "if model_name.find(\"_G\") > 0:\n",
    "    norm = \"Gaussian\"\n",
    "elif model_name.find(\"_m\") > 0:\n",
    "    norm = \"mean\"\n",
    "else:\n",
    "    norm = None\n",
    "    use_norm = False\n",
    "\n",
    "image_list = ['090085_20170531.h5',\n",
    "              \n",
    "#               '090085_20160512.h5',\n",
    "              '090085_20160528.h5',\n",
    "#               '090085_20160512.h5',# cloudy\n",
    "              \n",
    "#               '090085_20150510.h5', # little bit cloudy\n",
    "#               '090085_20150526.h5', => too much cloud\n",
    "              '090085_20150611.h5',\n",
    "\n",
    "#               '090085_20140320.h5',\n",
    "#               '090085_20140405.h5',\n",
    "#               '090085_20140421.h5',\n",
    "#               '090085_20140507.h5',# cloudy\n",
    "#               '090085_20140523.h5', => snow cover + too much cloud\n",
    "#               '090085_20140608.h5',\n",
    "#               '090085_20140624.h5',\n",
    "#                   '090085_20140710.h5', # might be alright...\n",
    "              '090085_20140726.h5',\n",
    "              \n",
    "              '090085_20130520.h5']\n",
    "image_list = sorted(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load raw image\n",
    "for image_name in image_list:\n",
    "    print(image_name)\n",
    "    image_path = path_image_dir + image_name\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "\n",
    "    show_raw_image(raw_image, size=-1)\n",
    "plt.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier ():\n",
    "    def __init__(self, path, name, classifier_type):\n",
    "        assert classifier_type in set(['LR', 'FCN'])\n",
    "        self.classifier_type = classifier_type\n",
    "        if classifier_type == 'LR':\n",
    "            self.classifier = joblib.load(path+name)\n",
    "            self.pos_idx = int(np.where(self.classifier.classes_ == 1)[0])\n",
    "            self.predict = lambda patch: self.classifier.predict_proba(patch.reshape((1, -1)))[0,self.pos_idx]\n",
    "\n",
    "        else: # FCN\n",
    "            tf.reset_default_graph()\n",
    "            tf.train.import_meta_graph(path+name+'.meta')\n",
    "            sess = tf.InteractiveSession()\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, path+name)\n",
    "            \n",
    "            graph = tf.get_default_graph()\n",
    "            self.x = graph.get_tensor_by_name(\"input/x:0\")\n",
    "            self.is_training = graph.get_tensor_by_name(\"input/is_training:0\")\n",
    "            self.prob_out = graph.get_tensor_by_name(\"prob_out/prob_out:0\")\n",
    "            \n",
    "            self.predict = lambda patch: self.prob_out.eval(feed_dict={self.x: patch.transpose((0, 2, 3, 1)), \n",
    "                                                                       self.is_training: False})[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Segmentation-FCN/Result/Inception/Incep_3-32;1-32|3-64;1-64_m_weight_bn_p0_e20_r0/Incep_3-32;1-32|3-64;1-64_m_weight_bn_p0_e20_r0\n"
     ]
    }
   ],
   "source": [
    "# re-load classifier\n",
    "if classifier_type == 'LR':\n",
    "    is_valid = lambda patch: (patch != -9999).all()\n",
    "    \n",
    "else:\n",
    "    assert classifier_type == 'FCN'\n",
    "    is_valid = lambda patch: ((patch==-9999).sum() / np.prod(np.array(patch.shape))) < 1/100\n",
    "\n",
    "classifier = Classifier(path_model_dir, model_name, classifier_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Time-Series/Pred/Incep_3-32;1-32|3-64;1-64_m_weight_bn_p0_e20_r0.h5\n",
      "mu =  [-0.00193307 -0.00129789  0.00198825  0.00165098 -0.00051675  0.00051223\n",
      "  0.00119147]\n",
      "(7961, 8091, 2) 090085_20130520.h5 20130520\n",
      "[]\n",
      "mu =  [-0.00177715 -0.00112118 -0.00110293 -0.00525456  0.00438297 -0.0016226\n",
      "  0.00298007]\n",
      "(7961, 8091, 2) 090085_20140726.h5 20140726\n",
      "[('20130520', <HDF5 dataset \"20130520\": shape (7961, 8091, 2), type \"<f8\">)]\n",
      "mu =  [ 0.00033616  0.00113001  0.00035501 -0.00320866 -0.00216324 -0.00197344\n",
      " -0.0015663 ]\n",
      "(7961, 8091, 2) 090085_20150611.h5 20150611\n",
      "[('20130520', <HDF5 dataset \"20130520\": shape (7961, 8091, 2), type \"<f8\">), ('20140726', <HDF5 dataset \"20140726\": shape (7961, 8091, 2), type \"<f8\">)]\n",
      "mu =  [-0.00166324  0.0017417  -0.00267226 -0.00253265 -0.00369252  0.00222735\n",
      " -0.00384184]\n",
      "(7961, 8091, 2) 090085_20160528.h5 20160528\n",
      "[('20130520', <HDF5 dataset \"20130520\": shape (7961, 8091, 2), type \"<f8\">), ('20140726', <HDF5 dataset \"20140726\": shape (7961, 8091, 2), type \"<f8\">), ('20150611', <HDF5 dataset \"20150611\": shape (7961, 8091, 2), type \"<f8\">)]\n",
      "mu =  [ 0.0019818   0.00022691  0.00284878 -0.00384517  0.00182871  0.00021076\n",
      "  0.002482  ]\n",
      "(7961, 8091, 2) 090085_20170531.h5 20170531\n",
      "[('20130520', <HDF5 dataset \"20130520\": shape (7961, 8091, 2), type \"<f8\">), ('20140726', <HDF5 dataset \"20140726\": shape (7961, 8091, 2), type \"<f8\">), ('20150611', <HDF5 dataset \"20150611\": shape (7961, 8091, 2), type \"<f8\">), ('20160528', <HDF5 dataset \"20160528\": shape (7961, 8091, 2), type \"<f8\">)]\n"
     ]
    }
   ],
   "source": [
    "path_pred_in_time = path_pred_dir + model_name + '.h5'\n",
    "print(path_pred_in_time)\n",
    "\n",
    "for image_name in image_list:\n",
    "    gc.collect()\n",
    "\n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    image_path = path_image_dir + image_name\n",
    "\n",
    "    # check prediction not existing\n",
    "    h5f = h5py.File(path_pred_in_time, 'a')\n",
    "    if date in set([pred for pred in h5f.keys()]):\n",
    "        print(image_path, 'already predicted, skip')\n",
    "        h5f.close()\n",
    "        continue\n",
    "    h5f.close()\n",
    "\n",
    "    # load raw image\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "    \n",
    "    # construct extractor\n",
    "    data_extractor = Pred_Data_Extractor(raw_image, step=step, size=size,\n",
    "                                         normalization=norm, is_valid=is_valid)\n",
    "    \n",
    "    # pred on image\n",
    "    if classifier_type == 'LR':\n",
    "        pred_road = np.zeros(shape=raw_image[0].shape)\n",
    "        print(pred_road.shape, image_name, date)\n",
    "\n",
    "        for coord, patch in data_extractor.iterate_raw_image_patches_with_coord(norm=use_norm):\n",
    "            pred_road[coord[0]+int(size/2),coord[1]+int(size/2)] = classifier.predict(patch)\n",
    "\n",
    "    else: # FCN        \n",
    "        pred_road = np.zeros(shape=(raw_image.shape[1], raw_image.shape[2], 2))\n",
    "        print(pred_road.shape, image_name, date)\n",
    "\n",
    "        for coord, patch in data_extractor.iterate_raw_image_patches_with_coord(norm=use_norm):\n",
    "            pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size] += classifier.predict(patch)\n",
    "            \n",
    "    # save prediction\n",
    "    h5f = h5py.File(path_pred_in_time, 'a')\n",
    "    print([i for i in h5f.items()])\n",
    "    h5f.create_dataset(name=date, data=pred_road)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze pred along the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(coord, mask, size):\n",
    "    return mask[coord[0]:coord[0]+size, coord[1]:coord[1]+size]\n",
    "\n",
    "def pred_normalization(pred):\n",
    "    pred_norm = pred[:,:,1]/pred.sum(axis=-1)\n",
    "    pred_norm[np.where(pred_norm != pred_norm)] = 0\n",
    "    pred_norm[np.where(pred_norm == np.float('inf'))] = 1\n",
    "    return pred_norm\n",
    "\n",
    "# softmax\n",
    "def pred_softmax(pred):\n",
    "    threshold = 500\n",
    "    pred_exp = pred.copy()\n",
    "    inf_idx = np.where(pred_exp > threshold)\n",
    "    \n",
    "    for x, y in zip(inf_idx[0], inf_idx[1]):\n",
    "        while((pred_exp[x,y] > threshold).any()):\n",
    "            pred_exp[x,y] = pred_exp[x,y] / 10\n",
    "    pred_exp = np.exp(pred_exp[:,:,1])/np.exp(pred_exp).sum(axis=-1)\n",
    "    pred_exp[np.where(pred[:,:,1] == 0)] = 0 # softmax([0,0]) = (0.5, 0.5)\n",
    "    return pred_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load road mask\n",
    "road_mask = skimage.io.imread(path_road_mask)\n",
    "road_proba = {}\n",
    "\n",
    "for image_name in image_list:\n",
    "    gc.collect()\n",
    "\n",
    "    # restore prediction \n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    h5f = h5py.File(path_pred_in_time, 'r')\n",
    "    pred_road = np.array(h5f[date])\n",
    "    h5f.close()\n",
    "\n",
    "    if classifier_type == 'FCN':\n",
    "        print('normalizing pred')\n",
    "        pred_road = pred_normalization(pred_road)\n",
    "    \n",
    "    # load raw image\n",
    "    image_path = path_image_dir + image_name\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "    \n",
    "    # construct extractor => scane through the image\n",
    "    data_extractor = Pred_Data_Extractor(raw_image, step=block_size, size=block_size,\n",
    "                                         normalization=None, is_valid=lambda patch: True)\n",
    "    assert not (date in road_proba)\n",
    "    road_proba[date] = {}\n",
    "    for coord, patch in data_extractor.iterate_raw_image_patches_with_coord(norm=False):\n",
    "        sub_valid_mask = (patch[0][0] != -9999)\n",
    "        sub_road_mask  = (get_patch(coord, road_mask, block_size) == 1)\n",
    "        sub_valid_road_mask = np.logical_and(sub_valid_mask, sub_road_mask)\n",
    "        \n",
    "        # patch should contain some roads\n",
    "        if sub_road_mask.sum() / np.prod(sub_road_mask.shape) < 0.1: continue\n",
    "#         if sub_valid_road_mask.sum() / np.prod(sub_valid_road_mask.shape) > 0.25:        \n",
    "\n",
    "        assert not (coord in road_proba[date])\n",
    "        mean_prob = get_patch(coord, pred_road, block_size)[np.where(sub_valid_road_mask)].mean()\n",
    "        road_proba[date][coord] = mean_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coord_list\n",
    "coord_list = sorted(road_proba[date].keys())\n",
    "for image_name in image_list:\n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    assert ( coord_list == sorted(road_proba[date].keys()) )\n",
    "\n",
    "# get date_list\n",
    "date_list = sorted(road_proba.keys())\n",
    "\n",
    "# construct matrix [date][coord]\n",
    "sorted_road = np.array([[road_proba[date][coord] for coord in coord_list]\n",
    "                        for date in date_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot proba by date\n",
    "plt.figure(figsize=(20, 20))\n",
    "for row, coord in zip((sorted_road.T), coord_list):\n",
    "    valid_row = row[np.where(row == row)]\n",
    "    \n",
    "    # ensure the probability increase\n",
    "    if len(valid_row) > 0 and (np.diff(valid_row) >= 0).all() and (np.diff(valid_row) > 0).any():\n",
    "        plt.plot(row, label=str(coord))\n",
    "        \n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearing_road_coord_list = []\n",
    "for row, coord in zip((sorted_road.T), coord_list):\n",
    "    valid_row = row[np.where(row == row)]\n",
    "    \n",
    "    # ensure the probability increase\n",
    "    if len(valid_row) > 0 and (np.diff(valid_row) >= 0).all() and (np.diff(valid_row) > 0).any():\n",
    "        appearing_road_coord_list.append(coord)\n",
    "        \n",
    "print(len(appearing_road_coord_list), set(appearing_road_coord_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrows = len(appearing_road_coord_list)\n",
    "ncols = len(image_list)\n",
    "print(nrows, ncols)\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(60,240))\n",
    "\n",
    "# from 2014 to 2017\n",
    "for image_name, index in zip(sorted(image_list), range(1,nrows+1)):\n",
    "    \n",
    "    # restore prediction \n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    h5f = h5py.File(path_pred_in_time, 'r')\n",
    "    pred_road = np.array(h5f[date])\n",
    "    h5f.close()\n",
    "\n",
    "    # load raw image\n",
    "    image_path = path_image_dir + image_name\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # plot the scene in current date\n",
    "    for coord in appearing_road_coord_list:\n",
    "        plt.title(str(date) + ',' + str(coord))\n",
    "        plt.subplot(nrows, ncols, index)\n",
    "        show_pred_prob_with_raw(raw_image, pred_road, true_road=road_mask, \n",
    "                                coord=coord, size=block_size, show_plot=False)\n",
    "        index += ncols\n",
    "\n",
    "plt.savefig('changing_road')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_proba_divide = {}\n",
    "divide_num = 4\n",
    "step_size = int(block_size/divide_num)\n",
    "\n",
    "for image_name in image_list:\n",
    "    gc.collect()\n",
    "\n",
    "    # restore prediction \n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    h5f = h5py.File(path_pred_in_time, 'r')\n",
    "    pred_road = np.array(h5f[date])\n",
    "    h5f.close()\n",
    "\n",
    "    # load raw image\n",
    "    image_path = path_image_dir + image_name\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "\n",
    "    assert not (date in road_proba_divide)\n",
    "    \n",
    "    for coord in coord_list:\n",
    "        for offset_x in range(divide_num):\n",
    "            offset_x *= step_size\n",
    "\n",
    "            for offset_y in range(divide_num):\n",
    "                offset_y *= step_size\n",
    "                \n",
    "                coord[0] += offset_x\n",
    "                coord[1] += offset_y\n",
    "                \n",
    "                sub_valid_mask = (get_patch(coord, raw_image[0], step_size) != -9999)\n",
    "                sub_road_mask  = (get_patch(coord, road_mask, block_size) == 1)\n",
    "                sub_valid_road_mask = np.logical_and(sub_valid_mask, sub_road_mask)\n",
    "                \n",
    "                if sub_valid_road_mask.sum() / np.prod(sub_valid_road_mask.shape) <  1/100:\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_valid_road_mask.sum() / np.prod(sub_valid_road_mask.shape) < 1/100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load road mask\n",
    "road_mask = skimage.io.imread(path_road_mask)\n",
    "road_proba = {}\n",
    "\n",
    "for image_name in image_list:\n",
    "    gc.collect()\n",
    "\n",
    "    # restore prediction \n",
    "    date = image_name.split('.')[0].split('_')[-1]\n",
    "    h5f = h5py.File(path_pred_in_time, 'r')\n",
    "    pred_road = np.array(h5f[date])\n",
    "    h5f.close()\n",
    "\n",
    "    # load raw image\n",
    "    image_path = path_image_dir + image_name\n",
    "    h5f = h5py.File(image_path, 'r')\n",
    "    raw_image = np.array(h5f['scene'])\n",
    "    h5f.close()\n",
    "    \n",
    "    # construct extractor\n",
    "    data_extractor = Pred_Data_Extractor(raw_image, step=block_size, size=block_size,\n",
    "                                         normalization=None, is_valid=lambda patch: True)\n",
    "    assert not (date in road_proba)\n",
    "    road_proba[date] = {}\n",
    "    for coord, patch in data_extractor.iterate_raw_image_patches_with_coord(norm=False):\n",
    "        sub_valid_mask = patch[0][0] != -9999\n",
    "        sub_road_mask  = get_patch(coord, road_mask, block_size) == 1\n",
    "        sub_valid_road_mask = np.logical_and(sub_valid_mask, sub_road_mask)\n",
    "        \n",
    "        assert not (coord in road_proba[date])\n",
    "        mean_prob = get_patch(coord, pred_road, block_size)[np.where(sub_valid_road_mask)].mean()\n",
    "        if mean_prob != mean_prob:\n",
    "            mean_prob = 0\n",
    "        road_proba[date][coord] = mean_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze log pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_pred = -np.log(-pred_road + 1 + 1e-7)\n",
    "print(log_pred.min(), log_pred.max(), log_pred.mean())\n",
    "\n",
    "norm_log_pred = (log_pred - log_pred.min()) / (log_pred.max()-log_pred.min())\n",
    "print(norm_log_pred.min(), norm_log_pred.max(), norm_log_pred.mean())\n",
    "\n",
    "plt.hist(x=norm_log_pred.flatten(), bins=100, histtype='step')\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.hist(x=norm_log_pred[np.where(norm_log_pred>0)].flatten(), bins=100, histtype='step')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_road = \"../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/road_mask.tif\"\n",
    "road_mask = skimage.io.imread(path_road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred_road_against_raw(raw_image, norm_log_pred, road_mask, threshold=0.40, figsize=(150,150), show_plot=False,\n",
    "                        save_path=\"123\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array([[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]]), cmap='hot')\n",
    "plt.show()\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.imshow(pred_road, cmap=plt.get_cmap('hot'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array([[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]]), cmap='hot')\n",
    "plt.show()\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.imshow(np.log(pred_road + 1e-9), cmap='hot')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
