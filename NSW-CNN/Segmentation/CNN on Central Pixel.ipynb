{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Metric/')\n",
    "from Metric import *\n",
    "from Data_Extractor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Reorder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7961, 8091) (7961, 8091)\n",
      "-9999\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "path_raw_image = \"../../Data/090085/090085_20170531.h5\"\n",
    "path_road_mask = \"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert/motor_trunk_pri_sec_tert.tif\"\n",
    "path_topleft_coordinate = \"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert/topleft_coordinate.h5\"\n",
    "\n",
    "raw_image = np.array(h5py.File(path_raw_image)['scene'])\n",
    "road_mask = skimage.io.imread(path_road_mask)\n",
    "\n",
    "data = h5py.File(path_topleft_coordinate, 'r')\n",
    "\n",
    "pos_topleft_coord = np.array(data['positive_example'])\n",
    "neg_topleft_coord = np.array(data['negative_example'])\n",
    "data.close()\n",
    "\n",
    "print(raw_image.shape, road_mask.shape)\n",
    "print(raw_image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct training & test set\n",
    "pos_index_mask = np.arange(pos_topleft_coord.shape[0])\n",
    "neg_index_mask = np.arange(neg_topleft_coord.shape[0])\n",
    "\n",
    "np.random.shuffle(pos_index_mask)\n",
    "np.random.shuffle(neg_index_mask)\n",
    "\n",
    "# 0 - neg; 1 - pos\n",
    "train_index = [neg_index_mask[:int(neg_index_mask.size*0.75) ], pos_index_mask[:int(pos_index_mask.size*0.75) ]]\n",
    "test_index  = [neg_index_mask[ int(neg_index_mask.size*0.75):], pos_index_mask[ int(pos_index_mask.size*0.75):]]\n",
    "\n",
    "Train_Data = Data_Extractor (raw_image, road_mask, 28, \n",
    "                             pos_topleft_coord = pos_topleft_coord[train_index[1]], \n",
    "                             neg_topleft_coord = neg_topleft_coord[train_index[0]])\n",
    "\n",
    "Test_Data  = Data_Extractor (raw_image, road_mask, 28, \n",
    "                             pos_topleft_coord = pos_topleft_coord[test_index[1]], \n",
    "                             neg_topleft_coord = neg_topleft_coord[test_index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 28\n",
    "height = 28\n",
    "band = 7\n",
    "\n",
    "# Hyper parameters\n",
    "conv_out = [0, 64, 128, 192]\n",
    "last_conv_flatten = 4*4*conv_out[-1]\n",
    "layer_out = [0, 256, 128]\n",
    "\n",
    "class_output = 1 # number of possible classifications for the problem\n",
    "\n",
    "uni = True # Xavier (uniform / normal)\n",
    "\n",
    "keep_rate = 0.5 # need regularization => otherwise NaN appears inside CNN\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 9e-6\n",
    "iteration = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Normalization Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5488,) float64\n",
      "(5488,) float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize Parameters\n",
    "mu = 0\n",
    "sigma = 0\n",
    "step = width\n",
    "for img in Train_Data:\n",
    "    mu += img\n",
    "mu = mu / Train_Data.size\n",
    "\n",
    "for img in Train_Data:\n",
    "    sigma += (img-mu)**2\n",
    "sigma = sigma / Train_Data.size\n",
    "\n",
    "print(mu.shape,    mu.dtype)\n",
    "print(sigma.shape, sigma.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holders for inputs and outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, height, width, band], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, class_output], name='y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout\n",
    "is_training = tf.placeholder(tf.bool, name='phase') # batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1\n",
    "W_conv1 = tf.get_variable('W_conv1', shape = [3, 3, band, conv_out[1]], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv1 = tf.Variable(tf.zeros([conv_out[1]]))\n",
    "\n",
    "convolve1= tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "bn_1 = tf.contrib.layers.batch_norm(convolve1, center=True, scale=True, is_training=is_training)\n",
    "\n",
    "h_conv1 = tf.nn.relu(convolve1)\n",
    "conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Convolutional Layer 2\n",
    "W_conv2 = tf.get_variable(\"W_conv2\", shape = [3, 3, conv_out[1], conv_out[2]], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv2 = tf.Variable(tf.zeros([conv_out[2]]))\n",
    "\n",
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')+ b_conv2\n",
    "bn_2 = tf.contrib.layers.batch_norm(convolve2, center=True, scale=True, is_training=is_training)\n",
    "\n",
    "h_conv2 = tf.nn.relu(convolve2)\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Convolutional Layer 3\n",
    "W_conv3 = tf.get_variable(\"W_conv3\", shape = [3, 3, conv_out[2], conv_out[3]], \n",
    "                          initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv3 = tf.Variable(tf.zeros([conv_out[3]]))\n",
    "\n",
    "convolve3= tf.nn.conv2d(conv2, W_conv3, strides = [1, 1, 1, 1], padding='SAME')+ b_conv3\n",
    "bn_3 = tf.contrib.layers.batch_norm(convolve3, center=True, scale=True, is_training=is_training)\n",
    "\n",
    "h_conv3 = tf.nn.relu(convolve3)\n",
    "conv3 = tf.nn.max_pool(h_conv3, ksize=[1,2,2,1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Flattening\n",
    "layer2_matrix = tf.reshape(conv3, [-1,last_conv_flatten])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "W_fc1 = tf.get_variable('W_fc1', shape = [last_conv_flatten, layer_out[1]], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_fc1 = tf.Variable(tf.zeros([layer_out[1]]))\n",
    "\n",
    "fcl=tf.matmul(layer2_matrix, W_fc1) + b_fc1\n",
    "h_fc1 = tf.nn.relu(fcl)\n",
    "\n",
    "# Drop out layer:\n",
    "layer_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Layer 2\n",
    "W_fc2 = tf.get_variable('W_fc2', shape = [layer_out[1], layer_out[2]], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_fc2 = tf.Variable(tf.zeros([layer_out[2]]))\n",
    "\n",
    "fc2=tf.matmul(layer_drop, W_fc2) + b_fc2\n",
    "h_fc2 = tf.nn.relu(fc2) # ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer (Sigmoid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc_out = tf.get_variable('W_fc_out', shape = [layer_out[2], class_output], \n",
    "                        initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "\n",
    "b_fc_out = tf.Variable(tf.zeros([class_output]))\n",
    "\n",
    "fc_out = tf.matmul(h_fc2, W_fc_out) + b_fc_out\n",
    "\n",
    "y_CNN = tf.sigmoid(fc_out, name='y_CNN')\n",
    "prediction = tf.cast(tf.round(y_CNN), tf.int32, name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function & optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum((y * tf.log(y_CNN) + (1-y) * tf.log(1-y_CNN)), axis=1))\n",
    "\n",
    "# Ensures that we execute the update_ops before performing the train_step\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y, tf.round(y_CNN)), \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & monitor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc =  0.875  cross entropy =  0.693084\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "learning_curve = []\n",
    "for i in range(iteration):\n",
    "\n",
    "    # force batch contains at least 1 positive example\n",
    "    batch_x, batch_y = Train_Data.get_patches(batch_size=64, positive_num=8)\n",
    "    batch = [((batch_x-mu)/sigma).reshape((-1,band,height,width)).transpose((0,2,3,1)).astype(np.float32), \n",
    "             np.matrix(batch_y).astype(int).T]\n",
    "\n",
    "    train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: keep_rate, is_training: True})\n",
    "        \n",
    "    # snap shot\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y: batch[1], keep_prob: 1})\n",
    "        learning_curve.append(train_accuracy)\n",
    "        \n",
    "        print(\"acc = \", train_accuracy, \" cross entropy = \", \n",
    "              cross_entropy.eval(feed_dict={x:batch[0], y: batch[1], keep_prob: 1}))\n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(y_CNN.eval(feed_dict={x:batch[0], keep_prob: 1}).T)\n",
    "print(fc_out.eval(feed_dict={x:batch[0], keep_prob: 0.5}).T)\n",
    "print(layer2_matrix.eval(feed_dict={x:batch[0]}).min())\n",
    "print(\"===\")\n",
    "print(conv3.eval(feed_dict={x:batch[0], keep_prob: 0.5}).flatten())\n",
    "print(conv2.eval(feed_dict={x:batch[0], keep_prob: 0.5}).flatten())\n",
    "print(conv1.eval(feed_dict={x:batch[0], keep_prob: 0.5}).flatten())\n",
    "print(convolve1.eval(feed_dict={x:batch[0], keep_prob: 0.5}).flatten())\n",
    "print(W_conv1.eval().flatten())\n",
    "print('=====')\n",
    "print(batch[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training curve\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(learning_curve)\n",
    "plt.title('learning_curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = './Result/motor_trunk_pri_sec_tert/'\n",
    "model_name = 'motor_trunk_pri_sec_tert'\n",
    "saver.save(sess, save_path + model_name)\n",
    "\n",
    "h5f = h5py.File(save_path + \"training_info.h5\", 'w')\n",
    "\n",
    "h5f_Index = h5f.create_group(\"Index\")\n",
    "h5f_Index.create_dataset(name='train_index', shape=train_index.shape, data=train_index)\n",
    "h5f_Index.create_dataset(name='test_index', shape=test_index.shape, data=test_index)\n",
    "\n",
    "h5f_Norm = h5f.create_group(\"Norm\")\n",
    "h5f_Norm.create_dataset(name='mu', shape=mu.shape, data=mu)\n",
    "h5f_Norm.create_dataset(name='sigma', shape=sigma.shape, data=sigma)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_metric = Metric()\n",
    "\n",
    "batch_num = int(train_index.size/batch_size)+1\n",
    "for i in range(batch_num):\n",
    "    start = i%batch_num * batch_size\n",
    "    end = start + batch_size\n",
    "\n",
    "    if end > train_index.size:\n",
    "        end = train_index.size\n",
    "    \n",
    "    index = train_index[start:end]\n",
    "    batch_x, batch_y = get_patches(raw_image, road_mask, topleft_coordinate[index], step=width)\n",
    "    batch = [((batch_x-mu)/sigma).reshape((-1,band,height,width)).transpose((0,2,3,1)).astype(np.float32), \n",
    "             np.matrix(batch_y).astype(int).T]\n",
    "    \n",
    "    # record metric\n",
    "    pred = prediction.eval(feed_dict={x:batch[0], keep_prob: 1, is_training: False})\n",
    "    train_metric.accumulate(pred, batch[1])\n",
    "    \n",
    "train_metric.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_metric = Metric()\n",
    "\n",
    "batch_num = int(test_index.size/batch_size)+1\n",
    "for i in range(batch_num):\n",
    "    start = i%batch_num * batch_size\n",
    "    end = start + batch_size\n",
    "\n",
    "    if end > test_index.size:\n",
    "        end = test_index.size\n",
    "    \n",
    "    index = test_index[start:end] \n",
    "    batch_x, batch_y = get_patches(raw_image, road_mask, topleft_coordinate[index], step=width)\n",
    "    batch = [((batch_x-mu)/sigma).reshape((-1,band,height,width)).transpose((0,2,3,1)).astype(np.float32), \n",
    "             np.matrix(batch_y).astype(int).T]\n",
    "    # record metric\n",
    "    pred = prediction.eval(feed_dict={x:batch[0], keep_prob: 1, is_training: False})\n",
    "    test_metric.accumulate(pred, batch[1])\n",
    "    \n",
    "test_metric.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
