{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skmt\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # so that plt works in command line\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "sys.path.append('../Metric/')\n",
    "sys.path.append('../../Visualization/')\n",
    "sys.path.append('../../Data_Preprocessing/')\n",
    "from Metric import *\n",
    "from Visualization import *\n",
    "from Data_Extractor import *\n",
    "from Bilinear_Kernel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: ../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_thr1_128_16_train\n",
      "CV set: ../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_thr1_128_16_cv\n",
      "will be saved as  FCN_16-32-64-128_3_cat1-16;1-32_weight_m_p0_e15_rNone\n",
      "will be saved into  ./Result/FCN/FCN_16-32-64-128_3_cat1-16;1-32_weight_m_p0_e15_rNone/\n",
      "mem usage before data loaded: 239.31640625 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = OptionParser()\n",
    "parser.add_option(\"--save\", dest=\"save_path\")\n",
    "parser.add_option(\"--name\", dest=\"model_name\")\n",
    "parser.add_option(\"--record_summary\", action=\"store_true\", default=False, dest=\"record_summary\")\n",
    "\n",
    "parser.add_option(\"--train\", dest=\"path_train_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_thr1_128_16_train\")\n",
    "parser.add_option(\"--cv\", dest=\"path_cv_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_thr1_128_16_cv\")\n",
    "\n",
    "parser.add_option(\"--norm\", default=\"mean\", dest=\"norm\")\n",
    "parser.add_option(\"--pos\", type=\"int\", default=0, dest=\"pos_num\")\n",
    "parser.add_option(\"--size\", type=\"int\", default=128, dest=\"size\")\n",
    "parser.add_option(\"-e\", \"--epoch\", type=\"int\", default=15, dest=\"epoch\")\n",
    "parser.add_option(\"--learning_rate\", type=\"float\", default=9e-6, dest=\"learning_rate\")\n",
    "parser.add_option(\"--batch\", type=\"int\", default=1, dest=\"batch_size\")\n",
    "parser.add_option(\"--rand\", type=\"int\", dest=\"rand_seed\")\n",
    "\n",
    "parser.add_option(\"--conv\", dest=\"conv_struct\")\n",
    "parser.add_option(\"--concat_input\", dest=\"concat_input\")\n",
    "parser.add_option(\"--not_weight\", action=\"store_false\", default=True, dest=\"use_weight\")\n",
    "parser.add_option(\"--use_batch_norm\", action=\"store_true\", default=False, dest=\"use_batch_norm\")\n",
    "parser.add_option(\"--output_conv\", type=\"int\", default=3, dest=\"output_conv\")\n",
    "\n",
    "parser.add_option(\"--gpu\", dest=\"gpu\")\n",
    "parser.add_option(\"--gpu_max_mem\", type=\"float\", default=0.8, dest=\"gpu_max_mem\")\n",
    "\n",
    "(options, args) = parser.parse_args([\"--gpu\", \"0\", \"--save\", \"./Result/FCN/\", \"--record_summary\",\n",
    "                                     \"--conv\", \"16-32-64-128\", \"--concat_input\", \"1-16;1-32\"])\n",
    "\n",
    "path_train_set = options.path_train_set\n",
    "path_cv_set = options.path_cv_set\n",
    "save_path = options.save_path\n",
    "model_name = options.model_name\n",
    "record_summary = options.record_summary\n",
    "\n",
    "norm = options.norm\n",
    "pos_num = options.pos_num\n",
    "size = options.size\n",
    "epoch = options.epoch\n",
    "batch_size = options.batch_size\n",
    "learning_rate = options.learning_rate\n",
    "rand_seed = options.rand_seed\n",
    "\n",
    "conv_struct = options.conv_struct\n",
    "\n",
    "use_weight = options.use_weight\n",
    "use_batch_norm = options.use_batch_norm\n",
    "concat_input = options.concat_input\n",
    "output_conv = options.output_conv\n",
    "\n",
    "gpu = options.gpu\n",
    "gpu_max_mem = options.gpu_max_mem\n",
    "\n",
    "# restrict to single gpu\n",
    "assert gpu in set(['0', '1'])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "if norm.startswith('m'): norm = 'mean'\n",
    "elif norm.startswith('G'): norm = 'Gaussian'\n",
    "else: \n",
    "    print(\"norm = \", norm, \" not in ('mean', 'Gaussian')\")\n",
    "    sys.exit()\n",
    "\n",
    "if not model_name:\n",
    "    model_name = \"FCN_\"\n",
    "    model_name += conv_struct + \"_\" + str(output_conv) + \"_\"\n",
    "    if concat_input: model_name += \"cat\" + concat_input + \"_\"\n",
    "    if use_weight: model_name += \"weight_\"\n",
    "    if use_batch_norm: model_name += \"bn_\"\n",
    "    model_name += norm[0] + \"_\"\n",
    "    model_name += \"p\" + str(pos_num) + \"_\"\n",
    "    model_name += \"e\" + str(epoch) + \"_\"\n",
    "    model_name += \"r\" + str(rand_seed)\n",
    "    \n",
    "if not save_path:\n",
    "    print(\"no save path provided\")\n",
    "    sys.exit()\n",
    "save_path = save_path.strip('/') + '/' + model_name + '/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(save_path+'Analysis'):\n",
    "    os.makedirs(save_path+'Analysis')\n",
    "\n",
    "print(\"Train set:\", path_train_set)\n",
    "print(\"CV set:\", path_cv_set)\n",
    "\n",
    "print(\"will be saved as \", model_name)\n",
    "print(\"will be saved into \", save_path)\n",
    "\n",
    "if not conv_struct:\n",
    "    print(\"must provide structure for conv\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    conv_struct = [int(x) for x in conv_struct.split('-')]\n",
    "    assert len(conv_struct) == 4\n",
    "\n",
    "# parse concat_input options (if not None): e.g. 3-16;5-8;1-32 \n",
    "# => concat[ 3x3 out_channel=16, 5x5 out_channel=8, 1x1 out_channel=32] followed by 1x1 conv out_channel = classoutput\n",
    "# concat_input = 0 => concat the raw input before the calculation of logits\n",
    "if concat_input:\n",
    "    concat_input = [[int(x) for x in config.split('-')] for config in concat_input.split(';')]\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage before data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 16], [1, 32]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data preparation '''\n",
    "\n",
    "\n",
    "\n",
    "# set random seed\n",
    "if not (rand_seed is None):\n",
    "    np.random.seed(rand_seed)\n",
    "\n",
    "# Load training set\n",
    "train_set = h5py.File(path_train_set, 'r')\n",
    "train_pos_topleft_coord = np.array(train_set['positive_example'])\n",
    "train_neg_topleft_coord = np.array(train_set['negative_example'])\n",
    "train_raw_image = np.array(train_set['raw_image'])\n",
    "train_road_mask = np.array(train_set['road_mask'])\n",
    "train_set.close()\n",
    "\n",
    "# Load cross-validation set\n",
    "CV_set = h5py.File(path_cv_set, 'r')\n",
    "CV_pos_topleft_coord = np.array(CV_set['positive_example'])\n",
    "CV_neg_topleft_coord = np.array(CV_set['negative_example'])\n",
    "CV_raw_image = np.array(CV_set['raw_image'])\n",
    "CV_road_mask = np.array(CV_set['road_mask'])\n",
    "CV_set.close()\n",
    "\n",
    "Train_Data = FCN_Data_Extractor (train_raw_image, train_road_mask, size,\n",
    "                                 pos_topleft_coord = train_pos_topleft_coord,\n",
    "                                 neg_topleft_coord = train_neg_topleft_coord,\n",
    "                                 normalization = norm)\n",
    "\n",
    "CV_Data = FCN_Data_Extractor (CV_raw_image, CV_road_mask, size,\n",
    "                              pos_topleft_coord = CV_pos_topleft_coord,\n",
    "                              neg_topleft_coord = CV_neg_topleft_coord,\n",
    "                              normalization = norm)\n",
    "# run garbage collector\n",
    "gc.collect()\n",
    "\n",
    "print(\"train data:\", train_raw_image.shape, train_road_mask.shape)\n",
    "print(\"cv data:\", CV_raw_image.shape, CV_road_mask.shape)\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create model '''\n",
    "\n",
    "\n",
    "\n",
    "# general model parameter\n",
    "band = 7\n",
    "\n",
    "class_output = 2 # number of possible classifications for the problem\n",
    "if use_weight:\n",
    "    class_weight = [Train_Data.pos_size/Train_Data.size, Train_Data.neg_size/Train_Data.size]\n",
    "    print(class_weight, '[neg, pos]')\n",
    "\n",
    "iteration = int(Train_Data.size/batch_size) + 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, size, size, band], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, size, size, class_output], name='y')\n",
    "    weight      = tf.placeholder(tf.float32, shape=[None, size, size], name='weight')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training') # batch norm\n",
    "\n",
    "if use_batch_norm:\n",
    "    normalizer_fn=tf.contrib.layers.batch_norm\n",
    "    normalizer_params={'scale':True, 'is_training':is_training}\n",
    "else:\n",
    "    normalizer_fn=None\n",
    "    normalizer_params=None\n",
    "\n",
    "with tf.variable_scope('input_bridge'):\n",
    "    if concat_input:\n",
    "        if concat_input == [[0]]:\n",
    "            input_map = x\n",
    "        else:\n",
    "            input_map = tf.concat([tf.contrib.layers.conv2d(inputs=x, num_outputs=cfg[1], kernel_size=cfg[0], \n",
    "                                                            stride=1, padding='SAME', scope=str(cfg[0])+'-'+str(cfg[1])) \n",
    "                                   for cfg in concat_input], \n",
    "                                  axis=-1)\n",
    "\n",
    "        \n",
    "\n",
    "with tf.variable_scope('down_sampling'):\n",
    "    # Convolutional Layer 1\n",
    "    net = tf.contrib.layers.conv2d(inputs=x, num_outputs=conv_struct[0], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv1')\n",
    "\n",
    "    conv1 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool1')\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[1], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv2')\n",
    "    conv2 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool2')\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[2], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv3')\n",
    "    conv3 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool3')\n",
    "\n",
    "\n",
    "net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[3], kernel_size=3, \n",
    "                               stride=1, padding='SAME', normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='bridge')\n",
    "\n",
    "\n",
    "with tf.variable_scope('up_sampling'):\n",
    "    kernel_size = get_kernel_size(2)\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[2], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[3], conv_struct[2])), \n",
    "                                             normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv3_T')\n",
    "    with tf.variable_scope('concat3'):\n",
    "        net = tf.concat([net, conv3], axis=-1)\n",
    "\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[1], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[2], conv_struct[1])), \n",
    "                                             normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv2_T')\n",
    "    with tf.variable_scope('concat2'):\n",
    "        net = tf.concat([net, conv2], axis=-1)\n",
    "    \n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[0], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[1], conv_struct[0])), \n",
    "                                             normalizer_fn=normalizer_fn, normalizer_params=normalizer_params, scope='conv1_T')\n",
    "\n",
    "    with tf.variable_scope('concat1'):\n",
    "        net = tf.concat([net, conv1], axis=-1)\n",
    "\n",
    "        if concat_input:\n",
    "            with tf.variable_scope('concat_input'):\n",
    "                net = tf.concat([net, input_map], axis=-1)\n",
    "\n",
    "logits = tf.contrib.layers.conv2d(inputs=net, num_outputs=class_output, kernel_size=output_conv, stride=1, padding='SAME', scope='logits')\n",
    "\n",
    "with tf.variable_scope('prob_out'):\n",
    "    prob_out = tf.nn.softmax(logits, name='prob_out')\n",
    "\n",
    "with tf.variable_scope('cross_entropy'):\n",
    "    flat_logits = tf.reshape(logits, (-1, class_output), name='flat_logits')\n",
    "    flat_labels = tf.to_float(tf.reshape(y, (-1, class_output)), name='flat_labels')\n",
    "    flat_weight = tf.reshape(weight, [-1], name='flat_weight')\n",
    "\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(flat_labels, flat_logits, weights=flat_weight)\n",
    "\n",
    "# Ensures that we execute the update_ops before performing the train_step\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "if record_summary:            \n",
    "    with tf.variable_scope('summary'):\n",
    "        graph = tf.get_default_graph()\n",
    "\n",
    "        # conv layers params\n",
    "        conv_scopes = ['conv1', 'conv2', 'conv3', 'bridge', 'conv3_T', 'conv2_T', 'conv1_T']\n",
    "\n",
    "        for scope_name in conv_scopes:\n",
    "            target_tensors = ['/weights:0']\n",
    "            if use_batch_norm: target_tensors.extend(['/BatchNorm/gamma:0', '/BatchNorm/beta:0'])\n",
    "            else: target_tensors.append('/biases:0')\n",
    "            for tensor_name in target_tensors:\n",
    "                tensor_name = scope_name + tensor_name\n",
    "                cur_tensor = graph.get_tensor_by_name(tensor_name)\n",
    "                tensor_name = tensor_name.split(':')[0]\n",
    "                tf.summary.histogram(tensor_name, cur_tensor)\n",
    "                tf.summary.histogram('grad_'+tensor_name, tf.gradients(cross_entropy, [cur_tensor])[0])\n",
    "\n",
    "        # logits layer params\n",
    "        scope_name = 'logits'\n",
    "        target_tensors = ['/weights:0', '/biases:0']\n",
    "        for tensor_name in target_tensors:\n",
    "            tensor_name = scope_name + tensor_name\n",
    "            cur_tensor = graph.get_tensor_by_name(tensor_name)\n",
    "            tensor_name = tensor_name.split(':')[0]\n",
    "            tf.summary.histogram(tensor_name, cur_tensor)\n",
    "            tf.summary.histogram('grad_'+tensor_name, tf.gradients(cross_entropy, [cur_tensor])[0])\n",
    "\n",
    "        # output layer\n",
    "        tf.summary.image('input', tf.reverse(x[:,:,:,1:4], axis=[-1])) # axis must be of rank 1\n",
    "        tf.summary.image('label', tf.expand_dims(y[:,:,:,1], axis=-1))\n",
    "        tf.summary.image('prob_out_pos', tf.expand_dims(prob_out[:,:,:,1], axis=-1))\n",
    "        tf.summary.image('prob_out_neg', tf.expand_dims(prob_out[:,:,:,0], axis=-1))\n",
    "        tf.summary.image('logits_pos', tf.expand_dims(logits[:,:,:,1], axis=-1))\n",
    "        tf.summary.image('logits_neg', tf.expand_dims(logits[:,:,:,0], axis=-1))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model created:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train & monitor '''\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpu_max_mem\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "balanced_acc_curve = []\n",
    "AUC_curve = []\n",
    "avg_precision_curve = []\n",
    "cross_entropy_curve = []\n",
    "for epoch_num in range(epoch):\n",
    "    for iter_num in range(iteration):\n",
    "\n",
    "        batch_x, batch_y, batch_w = Train_Data.get_patches(batch_size=batch_size, positive_num=pos_num, norm=True, weighted=use_weight)\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        train_step.run(feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: True})\n",
    "\n",
    "    if record_summary:\n",
    "        # tensor board\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        summary = sess.run(merged_summary, feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: False}, options=run_options, run_metadata=run_metadata)\n",
    "\n",
    "        train_writer.add_run_metadata(run_metadata, 'epoch_%03d' % (epoch_num+1))\n",
    "        train_writer.add_summary(summary, epoch_num+1)\n",
    "\n",
    "    # snap shot on CV set\n",
    "    cv_metric = Metric_Record()\n",
    "    cv_cross_entropy_list = []\n",
    "    for batch_x, batch_y, batch_w in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        [pred_prob, cross_entropy_cost] = sess.run([logits, cross_entropy], feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: False})\n",
    "        pred = int(pred_prob > 0.5)\n",
    "\n",
    "        cv_metric.accumulate(Y=batch_y, pred=pred, pred_prob=pred_prob)\n",
    "        cv_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "    # calculate value\n",
    "    balanced_acc = cv_metric.get_balanced_acc()\n",
    "    AUC_score = skmt.roc_auc_score(cv_metric.y_true, cv_metric.pred_prob)\n",
    "    avg_precision_score = skmt.average_precision_score(cv_metric.y_true, cv_metric.pred_prob)\n",
    "    mean_cross_entropy = sum(cv_cross_entropy_list)/len(cv_cross_entropy_list)\n",
    "\n",
    "    balanced_acc_curve.append(balanced_acc)\n",
    "    AUC_curve.append(AUC_score)\n",
    "    avg_precision_curve.append(avg_precision_score)\n",
    "    cross_entropy_curve.append(mean_cross_entropy)\n",
    "\n",
    "    print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "    sys.stdout.flush()\n",
    "print(\"finish\")\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model trained:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "\n",
    "# plot training curve\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(balanced_acc_curve, label='balanced_acc')\n",
    "plt.plot(AUC_curve, label='AUC')\n",
    "plt.plot(avg_precision_curve, label='avg_precision')\n",
    "plt.legend()\n",
    "plt.title('learning_curve_on_cross_validation')\n",
    "plt.savefig(save_path+'Analysis/'+'cv_learning_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(cross_entropy_curve)\n",
    "plt.title('cv_cross_entropy_curve')\n",
    "plt.savefig(save_path+'Analysis/'+'cv_cross_entropy_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# save model\n",
    "saver.save(sess, save_path + model_name)\n",
    "\n",
    "# run garbage collection\n",
    "saved_sk_obj = 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Evaluate model '''\n",
    "\n",
    "\n",
    "\n",
    "# train set eva\n",
    "print(\"On training set: \")\n",
    "train_metric = Metric_Record()\n",
    "train_cross_entropy_list = []\n",
    "for batch_x, batch_y, batch_w in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "    batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "    [pred_prob, cross_entropy_cost] = sess.run([logits, cross_entropy], feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: False})\n",
    "    pred = int(pred_prob > 0.5)\n",
    "    \n",
    "    train_metric.accumulate(Y=batch_y, pred=pred, pred_prob=pred_prob)    \n",
    "    train_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "train_metric.print_info()\n",
    "AUC_score = skmt.roc_auc_score(train_metric.y_true, train_metric.pred_prob)\n",
    "avg_precision_score = skmt.average_precision_score(train_metric.y_true, train_metric.pred_prob)\n",
    "mean_cross_entropy = sum(train_cross_entropy_list)/len(train_cross_entropy_list)\n",
    "print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(train_metric.y_true, train_metric.pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.savefig(save_path+'Analysis/'+'train_ROC_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# cross validation eva\n",
    "print(\"On CV set:\")\n",
    "cv_metric.print_info()\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(cv_metric.y_true, cv_metric.pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.savefig(save_path+'Analysis/'+'cv_ROC_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# run garbage collection\n",
    "train_metric = 0\n",
    "cv_metric = 0\n",
    "gc.collect()\n",
    "\n",
    "# Predict road mask\n",
    "# Predict road prob masks on train\n",
    "train_pred_road = np.zeros([x for x in train_road_mask.shape] + [2])\n",
    "for coord, patch in Train_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    train_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# Predict road prob on CV\n",
    "CV_pred_road = np.zeros([x for x in CV_road_mask.shape] + [2])\n",
    "for coord, patch in CV_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    CV_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# save prediction\n",
    "prediction_name = model_name + '_pred.h5'\n",
    "h5f_file = h5py.File(save_path + prediction_name, 'w')\n",
    "h5f_file.create_dataset (name='train_pred', data=train_pred_road)\n",
    "h5f_file.create_dataset (name='CV_pred', data=CV_pred_road)\n",
    "h5f_file.close()\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after prediction maps calculated:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
