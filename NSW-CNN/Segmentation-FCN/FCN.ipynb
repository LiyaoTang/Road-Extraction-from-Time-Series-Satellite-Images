{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Metric/')\n",
    "sys.path.append('../../Visualization/')\n",
    "sys.path.append('../../Data_Preprocessing/')\n",
    "from Metric import *\n",
    "from Data_Extractor import *\n",
    "from Bilinear_Kernel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Reorder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7961, 8091) (7961, 8091)\n",
      "-9999\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "path_raw_image = \"../../Data/090085/All_Data/090085_20170531.h5\"\n",
    "path_road_mask = \"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/motor_trunk_pri_sec_tert_uncl_track.tif\"\n",
    "path_topleft_coordinate = \"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_topleft_coordinate_8\"\n",
    "\n",
    "raw_image = np.array(h5py.File(path_raw_image)['scene'])\n",
    "road_mask = skimage.io.imread(path_road_mask)\n",
    "\n",
    "data = h5py.File(path_topleft_coordinate, 'r')\n",
    "\n",
    "pos_topleft_coord = np.array(data['positive_example'])\n",
    "neg_topleft_coord = np.array(data['negative_example'])\n",
    "data.close()\n",
    "\n",
    "print(raw_image.shape, road_mask.shape)\n",
    "print(raw_image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19131279 3826257\n"
     ]
    }
   ],
   "source": [
    "# Construct training & test set\n",
    "pos_size = pos_topleft_coord.shape[0]\n",
    "neg_size = neg_topleft_coord.shape[0]\n",
    "\n",
    "np.random.shuffle(pos_topleft_coord)\n",
    "np.random.shuffle(neg_topleft_coord)\n",
    "\n",
    "Train_Data = Data_Extractor (raw_image, road_mask, 8,\n",
    "                             pos_topleft_coord = pos_topleft_coord[:int(0.75*pos_size) ,:],\n",
    "                             neg_topleft_coord = neg_topleft_coord[:int(0.75*neg_size) ,:])\n",
    "\n",
    "Test_Data  = Data_Extractor (raw_image, road_mask, 8,\n",
    "                             pos_topleft_coord = pos_topleft_coord[ int(0.75*pos_size):int(0.90*pos_size),:],\n",
    "                             neg_topleft_coord = neg_topleft_coord[ int(0.75*neg_size):int(0.90*neg_size),:])\n",
    "\n",
    "print(Train_Data.size, Test_Data.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "band = 7\n",
    "\n",
    "# Hyper parameters\n",
    "conv_out = [0, 32, 64, 96]\n",
    "\n",
    "class_output = 2 # number of possible classifications for the problem\n",
    "\n",
    "use_weight = True\n",
    "keep_rate = 0.5 # need regularization => otherwise NaN appears inside CNN\n",
    "\n",
    "# class_weight = [Train_Data.pos_size/Train_Data.size, Train_Data.neg_size/Train_Data.size]\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 9e-6\n",
    "epoch = 15\n",
    "\n",
    "# print(class_weight, '[neg, pos]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[batch_size, size, size, band], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[batch_size, size, size, class_output], name='y')\n",
    "\n",
    "    weight = tf.placeholder(tf.float32, shape=[batch_size, size, size, class_output], name='class_weight')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob') # dropout\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training') # batch norm\n",
    "\n",
    "\n",
    "with tf.variable_scope('down_sampling'):\n",
    "    # Convolutional Layer 1\n",
    "    net = tf.contrib.layers.conv2d(inputs=x, num_outputs=conv_out[1], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv1')\n",
    "\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool1')\n",
    "    pool_1 = net\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_out[2], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv2')\n",
    "\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool2')\n",
    "    pool_2 = net\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_out[3], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv3')\n",
    "\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool3')\n",
    "\n",
    "\n",
    "with tf.variable_scope('up_sampling'):\n",
    "    kernel_size = get_kernel_size(2)\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_out[2], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_out[3], conv_out[2])), \n",
    "                                             scope='conv3_T')\n",
    "    with tf.variable_scope('fuse_with_2'):\n",
    "        net = net + pool_2\n",
    "\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_out[1], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_out[2], conv_out[1])), \n",
    "                                             scope='conv2_T')\n",
    "    with tf.variable_scope('fuse_with_1'):\n",
    "        net = net + pool_1\n",
    "    \n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=class_output, kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_out[1], class_output)), \n",
    "                                             scope='conv1_T')\n",
    "    pred = tf.nn.softmax(net)\n",
    "#     with tf.variable_scope('fuse_with_mean_input'):\n",
    "#         net = net + tf.reduce_mean(x, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function & optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/home/admin-u6142160/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "logits = tf.reshape(net, (-1, class_output))\n",
    "labels = tf.to_float(tf.reshape(y, (-1, class_output)))\n",
    "\n",
    "softmax = tf.nn.softmax(logits) + tf.constant(value=1e-7) # because of the numerical instableness\n",
    "\n",
    "if use_weight:\n",
    "    weight = tf.reshape(weight,(-1, class_output))\n",
    "    cross_entropy = -tf.reduce_sum(tf.multiply(labels * tf.log(softmax), weight) )\n",
    "else:\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(softmax), reduction_indices=[1])\n",
    "\n",
    "mean_cross_entropy = tf.reduce_mean(cross_entropy, name='mean_cross_entropy')\n",
    "\n",
    "# Ensures that we execute the update_ops before performing the train_step\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & monitor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "learning_curve = []\n",
    "iteration = int(Train_Data.size/batch_size) + 1\n",
    "\n",
    "for epoch_num in range(epoch):\n",
    "    for iter_num in range(iteration):\n",
    "        \n",
    "        batch_x, batch_y = Train_Data.get_patches(batch_size=64)\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "        train_step.run(feed_dict={x: batch_x, y: batch_y, keep_prob: keep_rate, is_training: True})\n",
    "\n",
    "    # snap shot\n",
    "    cur_cost = cross_entropy.eval(feed_dict={x:batch_x, y: batch_y, keep_prob: 1, is_training:False})\n",
    "    print(\"cross entropy = \", cur_cost)\n",
    "    learning_curve.append(cur_cost)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training curve\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(learning_curve)\n",
    "plt.title('learning_curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Result/motor_trunk_pri_sec_tert/CNN - central pixel/'\n",
    "model_name = 'motor_trunk_pri_sec_tert'\n",
    "saver.save(sess, save_path + model_name)\n",
    "\n",
    "h5f = h5py.File(save_path + \"training_info.h5\", 'w')\n",
    "\n",
    "h5f_Index = h5f.create_group(\"Index\")\n",
    "\n",
    "h5f_Index_Train = h5f_Index.create_group(name='Train')\n",
    "h5f_Index_Train.create_dataset (name='pos', data=Train_Data.pos_topleft_coord)\n",
    "h5f_Index_Train.create_dataset (name='neg', data=Train_Data.neg_topleft_coord)\n",
    "\n",
    "h5f_Index_Test  = h5f_Index.create_group(name='Test')\n",
    "h5f_Index_Test.create_dataset (name='pos', data=Test_Data.pos_topleft_coord)\n",
    "h5f_Index_Test.create_dataset (name='neg', data=Test_Data.neg_topleft_coord)\n",
    "\n",
    "h5f_Norm = h5f.create_group(\"Norm\")\n",
    "h5f_Norm.create_dataset(name='mu', shape=mu.shape, data=mu)\n",
    "h5f_Norm.create_dataset(name='sigma', shape=sigma.shape, data=sigma)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = Metric()\n",
    "\n",
    "for img, mask in Train_Data.iterate_data():\n",
    "    batch = [((img-mu)/sigma).reshape((-1,band,height,width)).transpose((0,2,3,1)).astype(np.float32), \n",
    "             np.matrix(mask).astype(int).T]\n",
    "\n",
    "    # record metric\n",
    "    pred = prediction.eval(feed_dict={x:batch[0], keep_prob: 1, is_training: False})\n",
    "    train_metric.accumulate(pred, batch[1])\n",
    "    \n",
    "train_metric.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metric = Metric()\n",
    "\n",
    "for img, mask in Test_Data.iterate_data():\n",
    "    batch = [((img-mu)/sigma).reshape((-1,band,height,width)).transpose((0,2,3,1)).astype(np.float32), \n",
    "             np.matrix(mask).astype(int).T]\n",
    "    \n",
    "    # record metric\n",
    "    pred = prediction.eval(feed_dict={x:batch[0], keep_prob: 1, is_training: False})\n",
    "    test_metric.accumulate(pred, batch[1])\n",
    "    \n",
    "test_metric.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
