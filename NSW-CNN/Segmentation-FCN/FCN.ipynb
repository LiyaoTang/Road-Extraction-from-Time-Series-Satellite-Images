{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skmt\n",
    "import matplotlib\n",
    "# matplotlib.use('agg') # so that plt works in command line\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "sys.path.append('../Metric/')\n",
    "sys.path.append('../../Visualization/')\n",
    "sys.path.append('../../Data_Preprocessing/')\n",
    "from Metric import *\n",
    "from Visualization import *\n",
    "from Data_Extractor import *\n",
    "from Bilinear_Kernel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = OptionParser()\n",
    "parser.add_option(\"--save\", dest=\"save_path\")\n",
    "parser.add_option(\"--name\", dest=\"model_name\")\n",
    "\n",
    "parser.add_option(\"--train\", dest=\"path_train_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_128_18_train\")\n",
    "parser.add_option(\"--cv\", dest=\"path_cv_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_128_18_cv\")\n",
    "\n",
    "parser.add_option(\"--norm\", default=\"mean\", dest=\"norm\")\n",
    "parser.add_option(\"--pos\", type=\"int\", default=0, dest=\"pos_num\")\n",
    "parser.add_option(\"--size\", type=\"int\", default=128, dest=\"size\")\n",
    "parser.add_option(\"-e\", \"--epoch\", type=\"int\", default=15, dest=\"epoch\")\n",
    "parser.add_option(\"--learning_rate\", type=\"float\", default=9e-6, dest=\"learning_rate\")\n",
    "parser.add_option(\"--batch\", type=\"int\", default=2, dest=\"batch_size\")\n",
    "parser.add_option(\"--rand\", type=\"int\", default=0, dest=\"rand_seed\")\n",
    "\n",
    "parser.add_option(\"--conv\", dest=\"conv_struct\")\n",
    "parser.add_option(\"--concat_input\", dest=\"concat_input\")\n",
    "parser.add_option(\"--not_weight\", action=\"store_false\", default=True, dest=\"use_weight\")\n",
    "parser.add_option(\"--use_batch_norm\", action=\"store_true\", default=False, dest=\"use_batch_norm\")\n",
    "\n",
    "parser.add_option(\"--gpu\", dest=\"gpu\")\n",
    "parser.add_option(\"--gpu_max_mem\", type=\"float\", default=0.8, dest=\"gpu_max_mem\")\n",
    "\n",
    "(options, args) = parser.parse_args([\"--save\", \"-\", \n",
    "                                     \"--gpu\", \"0\",\n",
    "                                     \"--conv\", \"16-32-64-128\", \n",
    "                                     \"--concat_input\", \"0\"])\n",
    "\n",
    "path_train_set = options.path_train_set\n",
    "path_cv_set = options.path_cv_set\n",
    "save_path = options.save_path\n",
    "model_name = options.model_name\n",
    "\n",
    "norm = options.norm\n",
    "pos_num = options.pos_num\n",
    "size = options.size\n",
    "epoch = options.epoch\n",
    "batch_size = options.batch_size\n",
    "learning_rate = options.learning_rate\n",
    "rand_seed = options.rand_seed\n",
    "\n",
    "conv_struct = options.conv_struct\n",
    "\n",
    "use_weight = options.use_weight\n",
    "use_batch_norm = options.use_batch_norm\n",
    "concat_input = options.concat_input\n",
    "\n",
    "gpu = options.gpu\n",
    "gpu_max_mem = options.gpu_max_mem\n",
    "\n",
    "# restrict to single gpu\n",
    "assert gpu in set(['0', '1'])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "if norm.startswith('m'): norm = 'mean'\n",
    "elif norm.startswith('G'): norm = 'Gaussian'\n",
    "else: \n",
    "    print(\"norm = \", norm, \" not in ('mean', 'Gaussian')\")\n",
    "    sys.exit()\n",
    "\n",
    "if not model_name:\n",
    "    model_name = \"Unet_\"\n",
    "    model_name += conv_struct + \"_\"\n",
    "    model_name += norm[0] + \"_\"\n",
    "    if use_weight: model_name += \"weight_\"\n",
    "    if use_batch_norm: model_name += \"bn_\"\n",
    "    if concat_input: model_name += \"concatI\" + concat_input + \"_\"\n",
    "    model_name += \"p\" + str(pos_num) + \"_\"\n",
    "    model_name += \"e\" + str(epoch) + \"_\"\n",
    "    model_name += \"r\" + str(rand_seed)\n",
    "    \n",
    "if not save_path:\n",
    "    print(\"no save path provided\")\n",
    "    sys.exit()\n",
    "save_path = save_path.strip('/') + '/' + model_name + '/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(save_path+'Analysis'):\n",
    "    os.makedirs(save_path+'Analysis')\n",
    "\n",
    "print(\"Train set:\", path_train_set)\n",
    "print(\"CV set:\", path_cv_set)\n",
    "\n",
    "print(\"will be saved as \", model_name)\n",
    "print(\"will be saved into \", save_path)\n",
    "\n",
    "if not conv_struct:\n",
    "    print(\"must provide structure for conv\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    conv_struct = [int(x) for x in conv_struct.split('-')]\n",
    "    assert len(conv_struct) == 4\n",
    "\n",
    "# parse concat_input options (if not None): e.g. 3-16;5-8;1-32 \n",
    "# => concat[ 3x3 out_channel=16, 5x5 out_channel=8, 1x1 out_channel=32] followed by 1x1 conv out_channel = classoutput\n",
    "# concat_input = 0 => concat the raw input before the calculation of logits\n",
    "if concat_input:\n",
    "    concat_input = [[int(x) for x in config.split('-')] for config in concat_input.split(';')]\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage before data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data preparation '''\n",
    "\n",
    "\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Load training set\n",
    "train_set = h5py.File(path_train_set, 'r')\n",
    "train_pos_topleft_coord = np.array(train_set['positive_example'])\n",
    "train_neg_topleft_coord = np.array(train_set['negative_example'])\n",
    "train_raw_image = np.array(train_set['raw_image'])\n",
    "train_road_mask = np.array(train_set['road_mask'])\n",
    "train_set.close()\n",
    "\n",
    "# Load cross-validation set\n",
    "CV_set = h5py.File(path_cv_set, 'r')\n",
    "CV_pos_topleft_coord = np.array(CV_set['positive_example'])\n",
    "CV_neg_topleft_coord = np.array(CV_set['negative_example'])\n",
    "CV_raw_image = np.array(CV_set['raw_image'])\n",
    "CV_road_mask = np.array(CV_set['road_mask'])\n",
    "CV_set.close()\n",
    "\n",
    "Train_Data = FCN_Data_Extractor (train_raw_image, train_road_mask, size,\n",
    "                                 pos_topleft_coord = train_pos_topleft_coord,\n",
    "                                 neg_topleft_coord = train_neg_topleft_coord,\n",
    "                                 normalization = norm)\n",
    "\n",
    "CV_Data = FCN_Data_Extractor (CV_raw_image, CV_road_mask, size,\n",
    "                              pos_topleft_coord = CV_pos_topleft_coord,\n",
    "                              neg_topleft_coord = CV_neg_topleft_coord,\n",
    "                              normalization = norm)\n",
    "# run garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Create model '''\n",
    "\n",
    "\n",
    "\n",
    "# general model parameter\n",
    "band = 7\n",
    "\n",
    "class_output = 2 # number of possible classifications for the problem\n",
    "if use_weight:\n",
    "    class_weight = [Train_Data.pos_size/Train_Data.size, Train_Data.neg_size/Train_Data.size]\n",
    "    print(class_weight, '[neg, pos]')\n",
    "\n",
    "iteration = int(Train_Data.size/batch_size) + 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, size, size, band], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, size, size, class_output], name='y')\n",
    "    weight      = tf.placeholder(tf.float32, shape=[None, size, size], name='weight')\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training') # batch norm\n",
    "\n",
    "with tf.variable_scope('input_bridge'):\n",
    "    if concat_input:\n",
    "        if concat_input == [[0]]:\n",
    "            input_map = x\n",
    "        else:\n",
    "            input_map = tf.concat([tf.contrib.layers.conv2d(inputs=x, num_outputs=cfg[1], kernel_size=cfg[0], \n",
    "                                                            stride=1, padding='SAME', scope=str(cfg[0])+'-'+str(cfg[1])) \n",
    "                                   for cfg in concat_input], \n",
    "                                  axis=-1)\n",
    "\n",
    "        \n",
    "\n",
    "with tf.variable_scope('down_sampling'):\n",
    "    # Convolutional Layer 1\n",
    "    net = tf.contrib.layers.conv2d(inputs=x, num_outputs=conv_struct[0], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv1')\n",
    "\n",
    "    conv1 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool1')\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[1], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv2')\n",
    "    conv2 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool2')\n",
    "    \n",
    "    # Convolutional Layer 3\n",
    "    net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[2], kernel_size=3, \n",
    "                                   stride=1, padding='SAME', scope='conv3')\n",
    "    conv3 = net\n",
    "    net = tf.contrib.layers.max_pool2d(inputs=net, kernel_size=2, stride=2, padding='VALID', scope='pool3')\n",
    "\n",
    "\n",
    "net = tf.contrib.layers.conv2d(inputs=net, num_outputs=conv_struct[3], kernel_size=3, \n",
    "                               stride=1, padding='SAME', scope='bridge')\n",
    "\n",
    "\n",
    "with tf.variable_scope('up_sampling'):\n",
    "    kernel_size = get_kernel_size(2)\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[2], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[3], conv_struct[2])), \n",
    "                                             scope='conv3_T')\n",
    "    with tf.variable_scope('concat3'):\n",
    "        net = tf.concat([net, conv3], axis=-1)\n",
    "\n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[1], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[2], conv_struct[1])), \n",
    "                                             scope='conv2_T')\n",
    "    with tf.variable_scope('concat2'):\n",
    "        net = tf.concat([net, conv2], axis=-1)\n",
    "    \n",
    "    net = tf.contrib.layers.conv2d_transpose(inputs=net, num_outputs=conv_struct[0], kernel_size=kernel_size, stride=2, \n",
    "                                             weights_initializer=tf.constant_initializer(get_bilinear_upsample_weights(2, conv_struct[1], conv_struct[0])), \n",
    "                                             scope='conv1_T')\n",
    "\n",
    "    with tf.variable_scope('concat1'):\n",
    "        net = tf.concat([net, conv1], axis=-1)\n",
    "\n",
    "        if concat_input:\n",
    "            with tf.variable_scope('concat_input'):\n",
    "                net = tf.concat([net, input_map], axis=-1)\n",
    "\n",
    "logits = tf.contrib.layers.conv2d(inputs=net, num_outputs=class_output, kernel_size=3, stride=1, padding='SAME', scope='logits')\n",
    "\n",
    "with tf.variable_scope('prob_out'):\n",
    "    prob_out = tf.nn.softmax(logits, name='prob_out')\n",
    "\n",
    "with tf.variable_scope('cross_entropy'):\n",
    "    flat_logits = tf.reshape(logits, (-1, class_output), name='flat_logits')\n",
    "    flat_labels = tf.to_float(tf.reshape(y, (-1, class_output)), name='flat_labels')\n",
    "    flat_weight = tf.reshape(weight, [-1], name='flat_weight')\n",
    "\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(flat_labels, flat_logits, weights=flat_weight)\n",
    "\n",
    "# Ensures that we execute the update_ops before performing the train_step\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model created:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train & monitor '''\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpu_max_mem\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "balanced_acc_curve = []\n",
    "AUC_curve = []\n",
    "avg_precision_curve = []\n",
    "cross_entropy_curve = []\n",
    "for epoch_num in range(epoch):\n",
    "    for iter_num in range(iteration):\n",
    "\n",
    "        batch_x, batch_y, batch_w = Train_Data.get_patches(batch_size=batch_size, positive_num=pos_num, norm=True, weighted=use_weight)\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        train_step.run(feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: True})\n",
    "        \n",
    "    # snap shot on CV set\n",
    "    cv_metric = Metric_Record()\n",
    "    cv_cross_entropy_list = []\n",
    "    for batch_x, batch_y, batch_w in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        [pred_prob, cross_entropy_cost] = sess.run([logits, cross_entropy], feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: False})\n",
    "\n",
    "        cv_metric.accumulate(Y         = np.array(batch_y.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                             pred      = np.array(pred_prob.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                             pred_prob = pred_prob.reshape(-1,class_output)[:,1])\n",
    "        cv_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "\n",
    "    # calculate value\n",
    "    balanced_acc = cv_metric.get_balanced_acc()\n",
    "    AUC_score = skmt.roc_auc_score(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "    avg_precision_score = skmt.average_precision_score(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "    mean_cross_entropy = sum(cv_cross_entropy_list)/len(cv_cross_entropy_list)\n",
    "\n",
    "    balanced_acc_curve.append(balanced_acc)\n",
    "    AUC_curve.append(AUC_score)\n",
    "    avg_precision_curve.append(avg_precision_score)\n",
    "    cross_entropy_curve.append(mean_cross_entropy)\n",
    "    \n",
    "    print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "    sys.stdout.flush()\n",
    "print(\"finish\")\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model trained:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "\n",
    "# plot training curve\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(balanced_acc_curve, label='balanced_acc')\n",
    "plt.plot(AUC_curve, label='AUC')\n",
    "plt.plot(avg_precision_curve, label='avg_precision')\n",
    "plt.legend()\n",
    "plt.title('learning_curve_on_cross_validation')\n",
    "plt.show()\n",
    "# plt.savefig(save_path+'Analysis/'+'cv_learning_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(cross_entropy_curve)\n",
    "plt.show()\n",
    "plt.title('cv_cross_entropy_curve')\n",
    "# plt.savefig(save_path+'Analysis/'+'cv_cross_entropy_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# save model\n",
    "saver.save(sess, save_path + model_name)\n",
    "\n",
    "# run garbage collection\n",
    "saved_sk_obj = 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Evaluate model '''\n",
    "\n",
    "\n",
    "\n",
    "# train set eva\n",
    "print(\"On training set: \")\n",
    "train_metric = Metric_Record()\n",
    "train_cross_entropy_list = []\n",
    "for batch_x, batch_y, batch_w in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "    batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "    [pred_prob, cross_entropy_cost] = sess.run([prob_out, cross_entropy], feed_dict={x: batch_x, y: batch_y, weight: batch_w, is_training: False})\n",
    "\n",
    "    train_metric.accumulate(Y         = np.array(batch_y.reshape(-1,class_output)[:,1]>0.5, dtype=int),\n",
    "                            pred      = np.array(pred_prob.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                            pred_prob = pred_prob.reshape(-1,class_output)[:,1])    \n",
    "    train_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "train_metric.print_info()\n",
    "AUC_score = skmt.roc_auc_score(np.array(train_metric.y_true).flatten(), np.array(train_metric.pred_prob).flatten())\n",
    "avg_precision_score = skmt.average_precision_score(np.array(train_metric.y_true).flatten(), np.array(train_metric.pred_prob).flatten())\n",
    "mean_cross_entropy = sum(train_cross_entropy_list)/len(train_cross_entropy_list)\n",
    "print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(np.array(train_metric.y_true)[:,1].flatten(), np.array(train_metric.pred_prob)[:,1].flatten())\n",
    "plt.plot(fpr, tpr)\n",
    "# plt.savefig(save_path+'Analysis/'+'train_ROC_curve.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# cross validation eva\n",
    "print(\"On CV set:\")\n",
    "cv_metric.print_info()\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "plt.plot(fpr, tpr)\n",
    "# plt.savefig(save_path+'Analysis/'+'cv_ROC_curve.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# run garbage collection\n",
    "train_metric = 0\n",
    "cv_metric = 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict road mask\n",
    "# Predict road prob masks on train\n",
    "train_pred_road = np.zeros([x for x in train_road_mask.shape] + [2])\n",
    "for coord, patch in Train_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    train_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# Predict road prob on CV\n",
    "CV_pred_road = np.zeros([x for x in CV_road_mask.shape] + [2])\n",
    "for coord, patch in CV_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    CV_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# save prediction\n",
    "prediction_name = model_name + '_pred.h5'\n",
    "h5f_file = h5py.File(save_path + prediction_name, 'w')\n",
    "h5f_file.create_dataset (name='train_pred', data=train_pred_road)\n",
    "h5f_file.create_dataset (name='CV_pred', data=CV_pred_road)\n",
    "h5f_file.close()\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after prediction maps calculated:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = (\"<stripped %d bytes>\"%size).encode('utf-8')\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
