{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skmt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import skimage.io\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "sys.path.append('../Metric/')\n",
    "sys.path.append('../../Visualization/')\n",
    "sys.path.append('../../Data_Preprocessing/')\n",
    "from Metric import *\n",
    "from Visualization import *\n",
    "from Data_Extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = OptionParser()\n",
    "parser.add_option(\"--save\", dest=\"save_path\")\n",
    "parser.add_option(\"--name\", dest=\"model_name\")\n",
    "\n",
    "parser.add_option(\"--train\", dest=\"path_train_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_128_18_train\")\n",
    "parser.add_option(\"--cv\", dest=\"path_cv_set\", default=\"../../Data/090085/Road_Data/motor_trunk_pri_sec_tert_uncl_track/posneg_seg_coord_split_128_18_cv\")\n",
    "\n",
    "parser.add_option(\"--norm\", default=\"mean\", dest=\"norm\")\n",
    "parser.add_option(\"--pos\", type=\"int\", default=0, dest=\"pos_num\")\n",
    "parser.add_option(\"--size\", type=\"int\", default=128, dest=\"size\")\n",
    "parser.add_option(\"-e\", \"--epoch\", type=\"int\", default=15, dest=\"epoch\")\n",
    "parser.add_option(\"--learning_rate\", type=\"float\", default=9e-6, dest=\"learning_rate\")\n",
    "parser.add_option(\"--batch\", type=\"int\", default=1, dest=\"batch_size\")\n",
    "parser.add_option(\"--rand\", type=\"int\", default=0, dest=\"rand_seed\")\n",
    "\n",
    "parser.add_option(\"--conv\", dest=\"conv_struct\")\n",
    "parser.add_option(\"--not_weight\", action=\"store_false\", default=True, dest=\"use_weight\")\n",
    "parser.add_option(\"--use_batch_norm\", action=\"store_true\", default=False, dest=\"use_batch_norm\")\n",
    "\n",
    "parser.add_option(\"--gpu\", dest=\"gpu\")\n",
    "parser.add_option(\"--gpu_max_mem\", type=\"float\", default=0.8, dest=\"gpu_max_mem\")\n",
    "\n",
    "(options, args) = parser.parse_args([\"--save\", \"./Result/Inception/\",\n",
    "                                     \"--rand\", \"0\",\n",
    "                                     \"--gpu\", \"0\",\n",
    "                                     \"--norm\", \"G\",\n",
    "                                     \"--not_weight\",\n",
    "                                     \"--conv\", \"3-16;5-8;1-32\"\n",
    "                                     ])\n",
    "\n",
    "path_train_set = options.path_train_set\n",
    "path_cv_set = options.path_cv_set\n",
    "save_path = options.save_path\n",
    "model_name = options.model_name\n",
    "\n",
    "pos_num = options.pos_num\n",
    "norm = options.norm\n",
    "size = options.size\n",
    "epoch = options.epoch\n",
    "batch_size = options.batch_size\n",
    "learning_rate = options.learning_rate\n",
    "rand_seed = options.rand_seed\n",
    "\n",
    "conv_struct = options.conv_struct\n",
    "\n",
    "use_weight = options.use_weight\n",
    "use_batch_norm = options.use_batch_norm\n",
    "\n",
    "gpu = options.gpu\n",
    "gpu_max_mem = options.gpu_max_mem\n",
    "\n",
    "# restrict to single gpu\n",
    "assert gpu in set(['0', '1'])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "if not save_path:\n",
    "    print(\"no save path provided\")\n",
    "    sys.exit()\n",
    "\n",
    "if norm.startswith('m'): norm = 'mean'\n",
    "elif norm.startswith('G'): norm = 'Gaussian'\n",
    "else: \n",
    "    print(\"norm = \", norm, \" not in ('mean', 'Gaussian')\")\n",
    "    sys.exit()\n",
    "\n",
    "if not model_name:\n",
    "    model_name = \"Incep_\"\n",
    "    model_name += conv_struct + \"_\"\n",
    "    model_name += norm[0] + \"_\"\n",
    "    if use_weight: model_name += \"weight_\"\n",
    "    if use_batch_norm: model_name += \"bn_\"\n",
    "    model_name += \"p\" + str(pos_num) + \"_\"\n",
    "    model_name += \"e\" + str(epoch) + \"_\"\n",
    "    model_name += \"r\" + str(rand_seed)\n",
    "    \n",
    "save_path = save_path.strip('/') + '/' + model_name + '/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(save_path+'Analysis'):\n",
    "    os.makedirs(save_path+'Analysis')\n",
    "\n",
    "print(\"Train set:\", path_train_set)\n",
    "print(\"CV set:\", path_cv_set)\n",
    "print(\"will be saved as \", model_name)\n",
    "print(\"will be saved into \", save_path)\n",
    "\n",
    "# parse conv_struct: e.g. 3-16;5-8;1-32 | 3-8;1-16 | ...\n",
    "# => concat[ 3x3 out_channel=16, 5x5 out_channel=8, 1x1 out_channel=32]\n",
    "# => followed by inception concat [3x3 out_channel=8, 1x1 out_channel=16]\n",
    "# => ...\n",
    "# conv_struct = 0 => use only one 1x1 conv out_channel = classoutput\n",
    "\n",
    "# note that at last layer, out_channel = 2 is requested\n",
    "if not conv_struct:\n",
    "    print(\"must provide structure for conv\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    conv_struct = [[[int(x) for x in config.split('-')] for config in layer.split(';')] for layer in conv_struct.split('|')]\n",
    "    print(\"conv_struct = \", conv_struct)\n",
    "    assert len(conv_struct) <= 3\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage before data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data preparation '''\n",
    "\n",
    "\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# Load training set\n",
    "train_set = h5py.File(path_train_set, 'r')\n",
    "train_pos_topleft_coord = np.array(train_set['positive_example'])\n",
    "train_neg_topleft_coord = np.array(train_set['negative_example'])\n",
    "train_raw_image = np.array(train_set['raw_image'])\n",
    "train_road_mask = np.array(train_set['road_mask'])\n",
    "train_set.close()\n",
    "\n",
    "# Load cross-validation set\n",
    "CV_set = h5py.File(path_cv_set, 'r')\n",
    "CV_pos_topleft_coord = np.array(CV_set['positive_example'])\n",
    "CV_neg_topleft_coord = np.array(CV_set['negative_example'])\n",
    "CV_raw_image = np.array(CV_set['raw_image'])\n",
    "CV_road_mask = np.array(CV_set['road_mask'])\n",
    "CV_set.close()\n",
    "\n",
    "Train_Data = FCN_Data_Extractor (train_raw_image, train_road_mask, size,\n",
    "                             pos_topleft_coord = train_pos_topleft_coord,\n",
    "                             neg_topleft_coord = train_neg_topleft_coord,\n",
    "                             normalization = norm)\n",
    "\n",
    "CV_Data = FCN_Data_Extractor (CV_raw_image, CV_road_mask, size,\n",
    "                          pos_topleft_coord = CV_pos_topleft_coord,\n",
    "                          neg_topleft_coord = CV_neg_topleft_coord,\n",
    "                          normalization = norm)\n",
    "# run garbage collector\n",
    "gc.collect()\n",
    "\n",
    "print(\"train data:\")\n",
    "print(train_raw_image.shape, train_road_mask.shape)\n",
    "print(\"pos = \", Train_Data.pos_size, \"neg = \", Train_Data.neg_size)\n",
    "print(\"cv data:\")\n",
    "print(CV_raw_image.shape, CV_road_mask.shape)\n",
    "print(\"pos = \", CV_Data.pos_size, \"neg = \", CV_Data.neg_size)\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after data loaded:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create model '''\n",
    "\n",
    "\n",
    "\n",
    "# general model parameter\n",
    "\n",
    "band = 7\n",
    "\n",
    "class_output = 2 # number of possible classifications for the problem\n",
    "if use_weight:\n",
    "    class_weight = [Train_Data.pos_size/Train_Data.size, Train_Data.neg_size/Train_Data.size]\n",
    "    print(class_weight, '[neg, pos]')\n",
    "\n",
    "iteration = int(Train_Data.size/batch_size) + 1\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, size, size, band], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, size, size, class_output], name='y')\n",
    "\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training') # batch norm\n",
    "\n",
    "if use_batch_norm:\n",
    "    normalizer_fn=tf.contrib.layers.batch_norm\n",
    "    normalizer_params={'scale':True, 'is_training':is_training}\n",
    "else:\n",
    "    normalizer_fn=None\n",
    "    normalizer_params=None\n",
    "\n",
    "with tf.variable_scope('inception'):\n",
    "    if conv_struct != [[[0]]]:\n",
    "        net = tf.concat([tf.contrib.layers.conv2d(inputs=x, num_outputs=cfg[1], kernel_size=cfg[0], stride=1, padding='SAME',\n",
    "                                                  normalizer_fn=normalizer_fn, normalizer_params=normalizer_params,\n",
    "                                                  scope=str(cfg[0])+'-'+str(cfg[1])) \n",
    "                         for cfg in conv_struct[0]], axis=-1)\n",
    "\n",
    "        if len(conv_struct) > 1:\n",
    "            for layer_cfg in conv_struct[1:]:\n",
    "                net = tf.concat([tf.contrib.layers.conv2d(inputs=net, num_outputs=cfg[1], kernel_size=cfg[0], stride=1, padding='SAME',\n",
    "                                                          normalizer_fn=normalizer_fn, normalizer_params=normalizer_params,\n",
    "                                                          scope=str(cfg[0])+'-'+str(cfg[1])) \n",
    "                                 for cfg in layer_cfg], axis=-1)\n",
    "\n",
    "    else:\n",
    "        net = x\n",
    "\n",
    "net = tf.contrib.layers.conv2d(inputs=net, num_outputs=class_output, kernel_size=1, stride=1, padding='SAME', scope='output_map')\n",
    "        \n",
    "with tf.variable_scope('logits'):\n",
    "    logits = tf.nn.softmax(net)\n",
    "\n",
    "with tf.variable_scope('cross_entropy'):\n",
    "    flat_logits = tf.reshape(logits, (-1, class_output))\n",
    "    labels = tf.to_float(tf.reshape(y, (-1, class_output)))\n",
    "\n",
    "    softmax = tf.nn.softmax(flat_logits) + tf.constant(value=1e-9) # because of the numerical instableness\n",
    "\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(softmax), reduction_indices=[1])\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy, name='mean_cross_entropy')\n",
    "\n",
    "# Ensures that we execute the update_ops before performing the train_step\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model created:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tf.get_default_graph().get_operations():\n",
    "    print(n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_scopes = ['output_map']\n",
    "for layer_cfg in conv_struct:\n",
    "    for cfg in layer_cfg:\n",
    "        conv_scopes.append('inception/' + str(cfg[0])+'-'+str(cfg[1]))\n",
    "        \n",
    "with tf.variable_scope('summary'):\n",
    "    graph = tf.get_default_graph()\n",
    "    for scope_name in conv_scopes:\n",
    "        for tensor_name in ['/weights', '/biases']:\n",
    "            tensor_name = scope_name + tensor_name + ':0'\n",
    "            cur_tensor = graph.get_tensor_by_name(tensor_name)\n",
    "            tensor_name = tensor_name.replace(':', '_')\n",
    "            tf.summary.histogram(tensor_name, cur_tensor)\n",
    "            tf.summary.histogram('grad_'+tensor_name, tf.gradients(cross_entropy, [cur_tensor])[0])\n",
    "            \n",
    "        tensor_name = scope_name+'/Relu:0'\n",
    "        cur_tensor = graph.get_tensor_by_name(tensor_name)\n",
    "        tensor_name = tensor_name.replace(':', '_')\n",
    "        tf.summary.tensor_summary(tensor_name, cur_tensor)\n",
    "        \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    tf.summary.image('logits', tf.expand_dims(logits[:,:,:,1], axis=-1))\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train & monitor '''\n",
    "\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpu_max_mem\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "balanced_acc_curve = []\n",
    "AUC_curve = []\n",
    "avg_precision_curve = []\n",
    "cross_entropy_curve = []\n",
    "train_writer = tf.summary.FileWriter('./Log/Inception/'+model_name, sess.graph)\n",
    "for epoch_num in range(epoch):\n",
    "    for iter_num in range(iteration):\n",
    "\n",
    "        batch_x, batch_y = Train_Data.get_patches(batch_size=batch_size, positive_num=pos_num, norm=True, weighted=use_weight)\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        train_step.run(feed_dict={x: batch_x, y: batch_y, is_training: True})\n",
    "       \n",
    "    # tensor board\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summary = sess.run(merged_summary, feed_dict={x: batch_x, y: batch_y, is_training: True}, options=run_options, run_metadata=run_metadata)\n",
    "\n",
    "    global_step = epoch_num*iteration + iter_num\n",
    "    train_writer.add_run_metadata(run_metadata, 'step%03d' % global_step)\n",
    "    train_writer.add_summary(summary, global_step)\n",
    "        \n",
    "    # snap shot on CV set\n",
    "    cv_metric = Metric_Record()\n",
    "    cv_cross_entropy_list = []\n",
    "    for batch_x, batch_y in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "        batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "        [pred_prob, cross_entropy_cost] = sess.run([logits, cross_entropy], feed_dict={x: batch_x, y: batch_y, is_training: False})\n",
    "\n",
    "        cv_metric.accumulate(Y         = np.array(batch_y.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                             pred      = np.array(pred_prob.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                             pred_prob = pred_prob.reshape(-1,class_output)[:,1])\n",
    "        cv_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "    # calculate value\n",
    "    balanced_acc = cv_metric.get_balanced_acc()\n",
    "    AUC_score = skmt.roc_auc_score(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "    avg_precision_score = skmt.average_precision_score(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "    mean_cross_entropy = sum(cv_cross_entropy_list)/len(cv_cross_entropy_list)\n",
    "\n",
    "    balanced_acc_curve.append(balanced_acc)\n",
    "    AUC_curve.append(AUC_score)\n",
    "    avg_precision_curve.append(avg_precision_score)\n",
    "    cross_entropy_curve.append(mean_cross_entropy)\n",
    "\n",
    "    print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "    sys.stdout.flush()\n",
    "print(\"finish\")\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after model trained:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()\n",
    "\n",
    "# plot training curve\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(balanced_acc_curve, label='balanced_acc')\n",
    "plt.plot(AUC_curve, label='AUC')\n",
    "plt.plot(avg_precision_curve, label='avg_precision')\n",
    "plt.legend()\n",
    "plt.title('learning_curve_on_cross_validation')\n",
    "plt.savefig(save_path+'Analysis/'+'cv_learning_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figsize=(9,5)\n",
    "plt.plot(cross_entropy_curve)\n",
    "plt.savefig(save_path+'Analysis/'+'cv_cross_entropy_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# save model\n",
    "saver.save(sess, save_path + model_name)\n",
    "\n",
    "# run garbage collection\n",
    "saved_sk_obj = 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Evaluate model '''\n",
    "\n",
    "\n",
    "\n",
    "# train set eva\n",
    "print(\"On training set: \")\n",
    "train_metric = Metric_Record()\n",
    "train_cross_entropy_list = []\n",
    "for batch_x, batch_y in CV_Data.iterate_data(norm=True, weighted=use_weight):\n",
    "    batch_x = batch_x.transpose((0, 2, 3, 1))\n",
    "\n",
    "    [pred_prob, cross_entropy_cost] = sess.run([logits, cross_entropy], feed_dict={x: batch_x, y: batch_y, is_training: False})\n",
    "\n",
    "    train_metric.accumulate(Y         = np.array(batch_y.reshape(-1,class_output)[:,1]>0.5, dtype=int),\n",
    "                            pred      = np.array(pred_prob.reshape(-1,class_output)[:,1]>0.5, dtype=int), \n",
    "                            pred_prob = pred_prob.reshape(-1,class_output)[:,1])    \n",
    "    train_cross_entropy_list.append(cross_entropy_cost)\n",
    "\n",
    "train_metric.print_info()\n",
    "AUC_score = skmt.roc_auc_score(np.array(train_metric.y_true).flatten(), np.array(train_metric.pred_prob).flatten())\n",
    "avg_precision_score = skmt.average_precision_score(np.array(train_metric.y_true).flatten(), np.array(train_metric.pred_prob).flatten())\n",
    "mean_cross_entropy = sum(train_cross_entropy_list)/len(train_cross_entropy_list)\n",
    "print(\"mean_cross_entropy = \", mean_cross_entropy, \"balanced_acc = \", balanced_acc, \"AUC = \", AUC_score, \"avg_precision = \", avg_precision_score)\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(np.array(train_metric.y_true)[:,1].flatten(), np.array(train_metric.pred_prob)[:,1].flatten())\n",
    "plt.plot(fpr, tpr)\n",
    "plt.savefig(save_path+'Analysis/'+'train_ROC_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# cross validation eva\n",
    "print(\"On CV set:\")\n",
    "cv_metric.print_info()\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thr = skmt.roc_curve(np.array(cv_metric.y_true).flatten(), np.array(cv_metric.pred_prob).flatten())\n",
    "plt.plot(fpr, tpr)\n",
    "plt.savefig(save_path+'Analysis/'+'cv_ROC_curve.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# run garbage collection\n",
    "train_metric = 0\n",
    "cv_metric = 0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict road mask\n",
    "# Predict road prob masks on train\n",
    "train_pred_road = np.zeros([x for x in train_road_mask.shape] + [2])\n",
    "for coord, patch in Train_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    train_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# Predict road prob on CV\n",
    "CV_pred_road = np.zeros([x for x in CV_road_mask.shape] + [2])\n",
    "for coord, patch in CV_Data.iterate_raw_image_patches_with_coord(norm=True):\n",
    "    patch = patch.transpose((0, 2, 3, 1))\n",
    "    CV_pred_road[coord[0]:coord[0]+size, coord[1]:coord[1]+size, :] += logits.eval(feed_dict={x: patch, is_training: False})[0]\n",
    "\n",
    "# save prediction\n",
    "prediction_name = model_name + '_pred.h5'\n",
    "h5f_file = h5py.File(save_path + prediction_name, 'w')\n",
    "h5f_file.create_dataset (name='train_pred', data=train_pred_road)\n",
    "h5f_file.create_dataset (name='CV_pred', data=CV_pred_road)\n",
    "h5f_file.close()\n",
    "\n",
    "# monitor mem usage\n",
    "process = psutil.Process(os.getpid())\n",
    "print('mem usage after prediction maps calculated:', process.memory_info().rss / 1024/1024, 'MB')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_road_norm = (train_pred_road[:,:,1] / train_pred_road.sum(axis=-1))\n",
    "train_pred_road_norm[np.where(train_pred_road_norm == np.float('inf'))] = 0\n",
    "train_pred_road_norm[np.where(train_pred_road_norm != train_pred_road_norm)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = Train_Data.get_patches(1, positive_num=1, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = batch_x[0]\n",
    "batch_y = batch_y[0, :, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred_prob_with_raw(batch_x, batch_y, show_plot=True, figsize=(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_patch = Train_Data.get_patches(1,positive_num=1, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred_prob_with_raw(example_patch[0][0], example_patch[1][0, :, :, 1],\n",
    "                            show_plot=True, figsize=(50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred\n",
    "# show_pred_prob_with_raw(train_raw_image, train_pred_road_norm, train_road_mask, pred_weight=5, figsize=(150,150), show_plot=False, save_path='./mask')\n",
    "show_pred_prob_with_raw(train_raw_image, train_pred_road_norm, train_road_mask, pred_weight=5, figsize=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
