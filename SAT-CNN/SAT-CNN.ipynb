{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Reorder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "SAT_4 = sio.loadmat('flatten_SAT_4.mat')\n",
    "train_x = SAT_4['train_x']\n",
    "train_y = SAT_4['train_y']\n",
    "index_mask = np.arange(train_x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "width = 28\n",
    "height = 28\n",
    "band = 4\n",
    "class_output = 4\n",
    "\n",
    "# Hyper parameters\n",
    "conv_out = [0, 64, 256, 512]\n",
    "            #   2048, 1024\n",
    "layer_out = [0, 1024, 512]\n",
    "last_conv_flatten = 4*4*conv_out[-1]\n",
    "\n",
    "keep_rate = 1.0\n",
    "\n",
    "batch_size = 64 # <- tuning\n",
    "learning_rate = 9e-6\n",
    "iteration = 50000\n",
    "\n",
    "# Initialization:\n",
    "# weight:\n",
    "uni = True # Xavier (uniform / normal)\n",
    "# bias: norm - mean=0, std=0.2  /// Xavier /// 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Normalization Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize Parameters\n",
    "Norm_Parameters = sio.loadmat(\"Feature_Norm.mat\")\n",
    "mu = Norm_Parameters[\"mu\"];\n",
    "sigma = Norm_Parameters[\"sigma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_i$ is the raw number (int)\n",
    "\n",
    "$\\mu = \\frac 1 n \\sum_{i=1}^n \\frac {a_i} {\\text{Max}} = \\frac 1 {\\text{Max}}(\\frac 1 n \\sum_{i=1}^n a_i)$\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac 1 n \\sum_{i=1}^n (\\frac{a_i}{\\text{Max}}-\\mu)^2 } = \\frac 1 {\\text{Max}} \\sqrt{\\frac 1 n \\sum_{i=1}^n(a_i-\\mu*\\text{Max})^2 } $\n",
    "\n",
    "$\\displaystyle \\frac {\\frac A {\\text{Max}} - mu} {\\sigma} = \\frac {A-\\mu *\\text{Max}}{\\sigma *\\text{Max}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place holders for inputs and outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32, shape=[None, width, height, band])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1\n",
    "Xavier_conv1 = np.sqrt(2/(3*3*band+conv_out[1]))\n",
    "# W_conv1 = tf.Variable(tf.truncated_normal([3, 3, 4, conv_out[1]], stddev=Xavier_conv1))\n",
    "# b_conv1 = tf.Variable(tf.truncated_normal([conv_out[1]], stddev=0.1))\n",
    "W_conv1 = tf.get_variable('W_conv1', shape = [3, 3, 4, conv_out[1]], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv1 = tf.Variable(tf.zeros([conv_out[1]]))\n",
    "\n",
    "convolve1= tf.nn.conv2d(x, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "h_conv1 = tf.nn.relu(convolve1)\n",
    "conv1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Convolutional Layer 2\n",
    "Xavier_conv2 = np.sqrt(2/(3*3*conv_out[1]+conv_out[2]))\n",
    "# W_conv2 = tf.Variable(tf.truncated_normal([3, 3, conv_out[1], conv_out[2]], stddev=Xavier_conv2))\n",
    "# b_conv2 = tf.Variable(tf.truncated_normal([conv_out[2]], stddev=0.1))\n",
    "W_conv2 = tf.get_variable(\"W_conv2\", shape = [3, 3, conv_out[1], conv_out[2]], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv2 = tf.Variable(tf.zeros([conv_out[2]]))\n",
    "\n",
    "convolve2= tf.nn.conv2d(conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME')+ b_conv2\n",
    "h_conv2 = tf.nn.relu(convolve2)\n",
    "conv2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Convolutional Layer 3\n",
    "Xavier_conv3 = np.sqrt(2/(3*3*conv_out[2]+conv_out[3]))\n",
    "# W_conv3 = tf.Variable(tf.truncated_normal([3, 3, conv_out[2], conv_out[3]], stddev=Xavier_conv3))\n",
    "# b_conv3 = tf.Variable(tf.truncated_normal([conv_out[3]], stddev=0.1))\n",
    "W_conv3 = tf.get_variable(\"W_conv3\", shape = [3, 3, conv_out[2], conv_out[3]], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_conv3 = tf.Variable(tf.zeros([conv_out[3]]))\n",
    "\n",
    "convolve3= tf.nn.conv2d(conv2, W_conv3, strides = [1, 1, 1, 1], padding='SAME')+ b_conv3\n",
    "h_conv3 = tf.nn.relu(convolve3)\n",
    "conv3 = tf.nn.max_pool(h_conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# Flattening\n",
    "layer2_matrix = tf.reshape(conv3, [-1,last_conv_flatten])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "Xavier_layer1 = np.sqrt(2/(last_conv_flatten+layer_out[1]))\n",
    "# W_fc1 = tf.Variable(tf.truncated_normal([last_conv_flatten, layer_out[1]], stddev=Xavier_layer1))\n",
    "# b_fc1 = tf.Variable(tf.truncated_normal([layer_out[1]], stddev=0.1))\n",
    "W_fc1 = tf.get_variable('W_fc1', shape = [last_conv_flatten, layer_out[1]], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_fc1 = tf.Variable(tf.zeros([layer_out[1]]))\n",
    "\n",
    "fcl=tf.matmul(layer2_matrix, W_fc1) + b_fc1\n",
    "h_fc1 = tf.nn.relu(fcl)\n",
    "\n",
    "# Drop out layer:\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "layer_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Layer 2\n",
    "Xavier_layer2 = np.sqrt(2/(layer_out[1]+layer_out[2]))\n",
    "# W_fc2 = tf.Variable(tf.truncated_normal([layer_out[1], layer_out[2]], stddev=Xavier_layer2)) # Xavier std = 0.044\n",
    "# b_fc2 = tf.Variable(tf.truncated_normal([layer_out[2]], stddev=0.1))\n",
    "W_fc2 = tf.get_variable('W_fc2', shape = [layer_out[1], layer_out[2]], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_fc2 = tf.Variable(tf.zeros([layer_out[2]]))\n",
    "\n",
    "fc2=tf.matmul(layer_drop, W_fc2) + b_fc2\n",
    "h_fc2 = tf.nn.relu(fc2) # ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer (Softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xavier_out = np.sqrt(2/(layer_out[2]+class_output))\n",
    "W_fc3 = tf.get_variable('W_fc3', shape = [layer_out[2], class_output], initializer=tf.contrib.layers.xavier_initializer(uniform=uni))\n",
    "b_fc3 = tf.Variable(tf.zeros([class_output]))\n",
    "# W_fc3 = tf.Variable(tf.truncated_normal([layer_out[2], class_output], stddev=Xavier_out)) # Xavier std = 0.0625\n",
    "# b_fc3 = tf.Variable(tf.truncated_normal([class_output], stddev=0.1))\n",
    "\n",
    "fc=tf.matmul(h_fc2, W_fc3) + b_fc3\n",
    "\n",
    "y_CNN= tf.nn.softmax(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function & optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * (y_CNN), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.141421356237 0.0490290337845 0.0266500895445 0.0147313912747 0.0360843918244 0.0622572806365\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_CNN,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(Xavier_conv1, Xavier_conv2, Xavier_conv3, Xavier_layer1, Xavier_layer2, Xavier_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & monitor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2304  147456 1179648 8388608  524288    2048] [  64  256  512 1024  512    4]\n",
      "10244352 2372\n"
     ]
    }
   ],
   "source": [
    "# record the initial weights & bias\n",
    "weight_init = np.concatenate((W_conv1.eval().flatten(), W_conv2.eval().flatten(), W_conv3.eval().flatten(), \n",
    "                             W_fc1.eval().flatten(), W_fc2.eval().flatten(), W_fc3.eval().flatten()))\n",
    "bias_init = np.concatenate((b_conv1.eval(), b_conv2.eval(), b_conv3.eval(),\n",
    "                            b_fc1.eval(), b_fc2.eval(), b_fc3.eval()))\n",
    "\n",
    "# get the weight & bias size\n",
    "weight_size = np.array((W_conv1.eval().flatten().size, W_conv2.eval().flatten().size, W_conv3.eval().flatten().size, \n",
    "                                W_fc1.eval().flatten().size, W_fc2.eval().flatten().size, W_fc3.eval().flatten().size))\n",
    "bias_size = np.array((b_conv1.eval().size, b_conv2.eval().size, b_conv3.eval().size, \n",
    "                            b_fc1.eval().size, b_fc2.eval().size, b_fc3.eval().size))\n",
    "\n",
    "print(weight_size, bias_size)\n",
    "print(weight_size.sum(), bias_size.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250.0\n",
      "start\n",
      "step 0      : train_acc = 0.109375 , cross entropy = -0.249868 \n",
      "step 1000   : train_acc = 0.625    , cross entropy = -0.624871 \n",
      "step 2000   : train_acc = 0.734375 , cross entropy = -0.719243 \n",
      "step 3000   : train_acc = 0.796875 , cross entropy = -0.792515 \n",
      "step 4000   : train_acc = 0.71875  , cross entropy = -0.722560 \n",
      "step 5000   : train_acc = 0.796875 , cross entropy = -0.796603 \n",
      "step 6000   : train_acc = 0.734375 , cross entropy = -0.740780 \n",
      "step 7000   : train_acc = 0.75     , cross entropy = -0.750154 \n",
      "step 8000   : train_acc = 0.765625 , cross entropy = -0.765365 \n",
      "step 9000   : train_acc = 0.671875 , cross entropy = -0.678844 \n",
      "step 10000  : train_acc = 0.828125 , cross entropy = -0.820032 \n",
      "step 11000  : train_acc = 0.796875 , cross entropy = -0.796709 \n",
      "step 12000  : train_acc = 0.703125 , cross entropy = -0.700867 \n",
      "step 13000  : train_acc = 0.6875   , cross entropy = -0.690441 \n",
      "step 14000  : train_acc = 0.8125   , cross entropy = -0.811715 \n",
      "step 15000  : train_acc = 0.703125 , cross entropy = -0.698669 \n",
      "step 16000  : train_acc = 0.796875 , cross entropy = -0.795990 \n",
      "step 17000  : train_acc = 0.8125   , cross entropy = -0.811938 \n",
      "step 18000  : train_acc = 0.75     , cross entropy = -0.750934 \n",
      "step 19000  : train_acc = 0.75     , cross entropy = -0.749894 \n",
      "step 20000  : train_acc = 0.765625 , cross entropy = -0.762272 \n",
      "step 21000  : train_acc = 0.71875  , cross entropy = -0.718886 \n",
      "step 22000  : train_acc = 0.734375 , cross entropy = -0.734061 \n",
      "step 23000  : train_acc = 0.84375  , cross entropy = -0.843706 \n",
      "step 24000  : train_acc = 0.75     , cross entropy = -0.742242 \n"
     ]
    }
   ],
   "source": [
    "print(len(train_x)/batch_size)\n",
    "batch_num = int(len(train_x)/batch_size)\n",
    "\n",
    "print(\"start\")\n",
    "learning_curve = []\n",
    "for i in range(iteration):\n",
    "    start = (i%batch_num) * batch_size\n",
    "    end = start + batch_size\n",
    "    train_index = index_mask[start:end]\n",
    "    \n",
    "    batch = [((train_x[train_index]-mu)/sigma).reshape(-1,width,height,band), train_y[train_index]]\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        learning_curve.append(train_accuracy)\n",
    "        print(\"step %-7d: train_acc = %-9g, cross entropy = %-10f\"\n",
    "              %(i, train_accuracy, cross_entropy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})))\n",
    "    \n",
    "    if i*batch_size % train_x.shape[0]:\n",
    "        np.random.shuffle(index_mask)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: keep_rate})\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Acc\n",
    "train_acc = []\n",
    "print(train_x.shape[0]/10)\n",
    "for i in range(int(train_x.shape[0]/10)):\n",
    "    start = i*10\n",
    "    end = start + 10\n",
    "    batch = [((train_x[start:end]-mu)/sigma).reshape(-1,width,height,band), train_y[start:end]]\n",
    "\n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    train_acc.append(train_accuracy)\n",
    "train_acc = sum(train_acc)/len(train_acc)\n",
    "print(\"Training Acc = \", train_acc)\n",
    "\n",
    "# Test Acc\n",
    "test_x = SAT_4['test_x']\n",
    "test_y = SAT_4['test_y']\n",
    "\n",
    "test_acc = []\n",
    "print(test_x.shape[0]/10)\n",
    "for i in range(int(test_x.shape[0]/10)):\n",
    "    start = i*10\n",
    "    end = start+10\n",
    "    test_acc.append(accuracy.eval(\n",
    "        feed_dict={x:((test_x[start:end]-mu)/sigma).reshape([-1,width,height,band]), \n",
    "                   y_:test_y[start:end], \n",
    "                   keep_prob:1.0})\n",
    "        )\n",
    "test_acc = sum(test_acc)/len(test_acc)\n",
    "print(\"Test Acc = \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Learning Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_curve = np.array(learning_curve)\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.plot(learning_curve)\n",
    "plt.axhline(0.8, color='r')\n",
    "plt.axhline(0.9, color='r')\n",
    "plt.axhline(1.0, color='r')\n",
    "plt.savefig('learning_curve - '+ str(learning_rate) + str(batch_size) + ' - ' + str(int(train_acc*100)) +'.png')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cmp weights & Bias; & Save fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record the trained weights & bias\n",
    "weight_trained = np.concatenate((W_conv1.eval().flatten(), W_conv2.eval().flatten(), W_conv3.eval().flatten(), \n",
    "                                W_fc1.eval().flatten(), W_fc2.eval().flatten(), W_fc3.eval().flatten()))\n",
    "bias_trained = np.concatenate((b_conv1.eval(), b_conv2.eval(), b_conv3.eval(),\n",
    "                               b_fc1.eval(), b_fc2.eval(), b_fc3.eval()))\n",
    "\n",
    "# calculate difference in weight & bias (before vs. after training)\n",
    "weight_diff = weight_trained - weight_init\n",
    "bias_diff = bias_trained - bias_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sio.savemat(\"init_model-\"+str(int(train_acc*100))+'.mat', {\"weight\":weight_init, \"bias\":bias_init})\n",
    "sio.savemat(\"trained_model-\"+str(int(train_acc*100))+'.mat', {\"weight\":weight_trained, \"bias\":bias_trained})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['agg.path.chunksize'] = 50000\n",
    "\n",
    "def plot_para(para, size, name):\n",
    "    plt.figure(figsize=size)\n",
    "    plt.scatter(range(para.size), para, marker='.')\n",
    "    acc_size = 0 # slice weights  \n",
    "    plt.axvline(acc_size, color='r')\n",
    "    for size in weight_size:\n",
    "        acc_size += size\n",
    "        plt.axvline(acc_size, color='r')\n",
    "        \n",
    "    plt.axhline(0.1, color='r')\n",
    "    plt.axhline(0, color='r')\n",
    "    plt.axhline(-0.1, color='r')\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.clf()\n",
    "\n",
    "plot_para(weight_init, (100, 30), \"weight_init\")\n",
    "plot_para(bias_init, (10,5), \"bias_init\")\n",
    "plot_para(weight_trained, (100, 30), \"weight_trained\")\n",
    "plot_para(bias_trained, (10,5), \"bias_trained\")\n",
    "plot_para(weight_diff, (100, 30), \"weight_diff\")\n",
    "plot_para(bias_diff, (10,5), \"bias_diff\")\n",
    "\n",
    "print(\"total diff in weights & bias = \", weight_diff.sum(), \" & \", bias_diff.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"init:\")\n",
    "print(weight_init.std(), weight_init.mean(), weight_init.min(), \"--\", weight_init.max())\n",
    "print(bias_init.std(), bias_init.mean(), bias_init.min(), \"--\", bias_init.max())\n",
    "print(\"\\ntrained:\")\n",
    "print(weight_trained.std(), weight_trained.mean(), weight_trained.min(), \"--\", weight_trained.max())\n",
    "print(bias_trained.std(), bias_trained.mean(), bias_trained.min(), \"--\", bias_trained.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close() #finish the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
